{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-07T06:33:16.753128Z","iopub.execute_input":"2023-05-07T06:33:16.753599Z","iopub.status.idle":"2023-05-07T06:33:16.760679Z","shell.execute_reply.started":"2023-05-07T06:33:16.753522Z","shell.execute_reply":"2023-05-07T06:33:16.759766Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install libauc==1.2.0\n!pip install medmnist\n!pip install torchio","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:33:21.427905Z","iopub.execute_input":"2023-05-07T06:33:21.428240Z","iopub.status.idle":"2023-05-07T06:33:51.715896Z","shell.execute_reply.started":"2023-05-07T06:33:21.428211Z","shell.execute_reply":"2023-05-07T06:33:51.714476Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport medmnist\nfrom medmnist import INFO, Evaluator\n\nfrom libauc.models import resnet18\nfrom libauc.sampler import DualSampler\nfrom libauc.metrics import auc_prc_score\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nfrom torchio import Image","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:33:51.718150Z","iopub.execute_input":"2023-05-07T06:33:51.718517Z","iopub.status.idle":"2023-05-07T06:33:56.524809Z","shell.execute_reply.started":"2023-05-07T06:33:51.718478Z","shell.execute_reply":"2023-05-07T06:33:56.523929Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:33:56.525980Z","iopub.execute_input":"2023-05-07T06:33:56.526663Z","iopub.status.idle":"2023-05-07T06:33:56.532743Z","shell.execute_reply.started":"2023-05-07T06:33:56.526616Z","shell.execute_reply":"2023-05-07T06:33:56.531834Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"MedMNIST v2.2.1 @ https://github.com/MedMNIST/MedMNIST/\n","output_type":"stream"}]},{"cell_type":"code","source":"#data_flag = 'breastmnist'\n#data_flag = 'pneumoniamnist'\n#data_flag = 'chestmnist'\n#data_flag = 'nodulemnist3d'\n#data_flag = 'adrenalmnist3d'\ndata_flag = 'vesselmnist3d'\n\ndownload = True\n\n#NUM_EPOCHS = 3\nBATCH_SIZE = 32\n#lr = 0.001\n\ninfo = INFO[data_flag]\ntask = info['task']\nn_channels = info['n_channels']\nn_classes = len(info['label'])\n\nDataClass = getattr(medmnist, info['python_class'])\n\nprint(info)\nprint(n_channels)\nprint(n_classes)\nprint(DataClass)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:33:56.535223Z","iopub.execute_input":"2023-05-07T06:33:56.536170Z","iopub.status.idle":"2023-05-07T06:33:56.544997Z","shell.execute_reply.started":"2023-05-07T06:33:56.536137Z","shell.execute_reply":"2023-05-07T06:33:56.543979Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'python_class': 'VesselMNIST3D', 'description': 'The VesselMNIST3D is based on an open-access 3D intracranial aneurysm dataset, IntrA, containing 103 3D models (meshes) of entire brain vessels collected by reconstructing MRA images. 1,694 healthy vessel segments and 215 aneurysm segments are generated automatically from the complete models. We fix the non-watertight mesh with PyMeshFix and voxelize the watertight mesh with trimesh into 28×28×28 voxels. We split the source dataset with a ratio of 7:1:2 into training, validation and test set.', 'url': 'https://zenodo.org/record/6496656/files/vesselmnist3d.npz?download=1', 'MD5': '2ba5b80617d705141f3f85627108fce8', 'task': 'binary-class', 'label': {'0': 'vessel', '1': 'aneurysm'}, 'n_channels': 1, 'n_samples': {'train': 1335, 'val': 192, 'test': 382}, 'license': 'CC BY 4.0'}\n1\n2\n<class 'medmnist.dataset.VesselMNIST3D'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchio.transforms import RandomAffine, RandomFlip, RandomNoise, RandomGamma\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, images, targets, image_size=28, crop_size=24, mode='train'):\n        self.images = images\n        self.targets = targets\n        self.mode = mode\n        \n        self.transform_train = transforms.Compose([RandomAffine(scales=(0.9, 1.2),\n                                                                degrees=15,\n                                                                p=0.5),\n#                                                    RandomAffine(),\n                                                   RandomGamma(log_gamma=(-0.3, 0.3), p=0.5),\n                                                   RandomFlip(p = 0.5),\n                                                   RandomNoise(p=0.2),\n                                                  ])\n        self.transform_test = transforms.Compose([RandomAffine(p=0),\n#                                                    RandomAffine(),\n                                                   RandomFlip(p=0),\n                                                   RandomNoise(p=0),\n                                                  ])\n        \n#         increased_dataset = torch.utils.data.ConcatDataset([transformed_dataset,original])\n        \n#         self.transform_train = transforms.Compose([                                                \n#                               transforms.ToTensor(),\n# #                               transforms.RandomCrop((crop_size, crop_size, crop_size), padding=None),\n#                               transforms.RandomHorizontalFlip(),\n# #                               transforms.Resize((image_size, image_size, image_size)),\n#                               ])\n#         self.transform_test = transforms.Compose([\n#                              transforms.ToTensor(),\n# #                              transforms.Resize((image_size, image_size, image_size)),\n# #                              transforms.Normalize(mean=[.5], std=[.5])\n#                               ])\n        \n        \n        # for loss function\n        self.pos_indices = np.flatnonzero(targets==1)\n        self.pos_index_map = {}\n        for i, idx in enumerate(self.pos_indices):\n            self.pos_index_map[idx] = i\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        target = self.targets[idx]\n#         image = Image.fromarray(image.astype('uint8'))\n        if self.mode == 'train':\n           idx = self.pos_index_map[idx] if idx in self.pos_indices else -1\n           image = self.transform_train(image)\n        else:\n           image = self.transform_test(image)\n        return idx, image, target ","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:33:56.546491Z","iopub.execute_input":"2023-05-07T06:33:56.547273Z","iopub.status.idle":"2023-05-07T06:33:56.558599Z","shell.execute_reply.started":"2023-05-07T06:33:56.547240Z","shell.execute_reply":"2023-05-07T06:33:56.557640Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = DataClass(split='train',  download=download)\n\n# encapsulate data into dataloader form\n#train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nval_dataset = DataClass(split='val',  download=download)\n#train_loader_at_eval = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = DataClass(split='test',  download=download)\n#test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:33:56.560295Z","iopub.execute_input":"2023-05-07T06:33:56.560695Z","iopub.status.idle":"2023-05-07T06:34:00.650936Z","shell.execute_reply.started":"2023-05-07T06:33:56.560662Z","shell.execute_reply":"2023-05-07T06:34:00.649749Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/record/6496656/files/vesselmnist3d.npz?download=1 to /root/.medmnist/vesselmnist3d.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 398373/398373 [00:00<00:00, 548082.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using downloaded and verified file: /root/.medmnist/vesselmnist3d.npz\nUsing downloaded and verified file: /root/.medmnist/vesselmnist3d.npz\n","output_type":"stream"}]},{"cell_type":"code","source":"train_images = np.array([np.asarray(image) for (image, target) in train_dataset])\ntrain_labels = np.array([target for (image, target) in train_dataset])\nprint(type(train_images[0]))\n\nval_images = np.array([np.asarray(image) for (image, target) in val_dataset])\nval_labels = np.array([target for (image, target) in val_dataset])\nprint(type(val_images[0]))\n\ntest_images = np.array([np.asarray(image) for (image, target) in test_dataset])\ntest_labels = [target for (image, target) in test_dataset]\nprint(type(test_images[0]))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:00.652152Z","iopub.execute_input":"2023-05-07T06:34:00.652768Z","iopub.status.idle":"2023-05-07T06:34:01.175515Z","shell.execute_reply.started":"2023-05-07T06:34:00.652732Z","shell.execute_reply":"2023-05-07T06:34:01.174408Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 32\nsampling_rate = 0.5\n\ntrainSet = ImageDataset(train_images, train_labels)\ntrainSet_eval = ImageDataset(val_images, val_labels, mode='test')\ntestSet = ImageDataset(test_images, test_labels, mode='test')\n\nsampler = DualSampler(dataset=trainSet, batch_size=batch_size, sampling_rate=sampling_rate)\ntrain_loader = torch.utils.data.DataLoader(trainSet, batch_size=batch_size, sampler=sampler, num_workers=2)\ntrain_loader_at_eval = torch.utils.data.DataLoader(trainSet_eval, batch_size=batch_size, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testSet, batch_size=batch_size, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.177409Z","iopub.execute_input":"2023-05-07T06:34:01.178566Z","iopub.status.idle":"2023-05-07T06:34:01.189477Z","shell.execute_reply.started":"2023-05-07T06:34:01.178526Z","shell.execute_reply":"2023-05-07T06:34:01.188636Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.190825Z","iopub.execute_input":"2023-05-07T06:34:01.191254Z","iopub.status.idle":"2023-05-07T06:34:01.197360Z","shell.execute_reply.started":"2023-05-07T06:34:01.191218Z","shell.execute_reply":"2023-05-07T06:34:01.196281Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<torch.utils.data.dataloader.DataLoader object at 0x7b5daab80c40>\n","output_type":"stream"}]},{"cell_type":"code","source":"x, y = train_dataset[0]\n\nprint(x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.202211Z","iopub.execute_input":"2023-05-07T06:34:01.202990Z","iopub.status.idle":"2023-05-07T06:34:01.208262Z","shell.execute_reply.started":"2023-05-07T06:34:01.202959Z","shell.execute_reply":"2023-05-07T06:34:01.207140Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(1, 28, 28, 28) (1,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# frames = train_dataset.montage(length=20)\n# frames[10]","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.209899Z","iopub.execute_input":"2023-05-07T06:34:01.210692Z","iopub.status.idle":"2023-05-07T06:34:01.215744Z","shell.execute_reply.started":"2023-05-07T06:34:01.210656Z","shell.execute_reply":"2023-05-07T06:34:01.214742Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"lables = train_dataset.labels\ncnt1 = 0\ncnt0 = 0\nfor i in range(len(lables)):\n  if lables[i][0] == 0:\n    cnt0+=1\n  else:\n    cnt1+=1\nprint(cnt0)\nprint(cnt1)\nprint(cnt0*100/(cnt0+cnt1))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.217449Z","iopub.execute_input":"2023-05-07T06:34:01.218278Z","iopub.status.idle":"2023-05-07T06:34:01.228816Z","shell.execute_reply.started":"2023-05-07T06:34:01.218236Z","shell.execute_reply":"2023-05-07T06:34:01.227799Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"1185\n150\n88.76404494382022\n","output_type":"stream"}]},{"cell_type":"code","source":"lables = test_dataset.labels\ncnt1 = 0\ncnt0 = 0\nfor i in range(len(lables)):\n  if lables[i][0] == 0:\n    cnt0+=1\n  else:\n    cnt1+=1\nprint(cnt0)\nprint(cnt1)\nprint(cnt0*100/(cnt0+cnt1))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.230161Z","iopub.execute_input":"2023-05-07T06:34:01.230487Z","iopub.status.idle":"2023-05-07T06:34:01.239352Z","shell.execute_reply.started":"2023-05-07T06:34:01.230457Z","shell.execute_reply":"2023-05-07T06:34:01.238299Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"339\n43\n88.7434554973822\n","output_type":"stream"}]},{"cell_type":"code","source":"from libauc.losses import AUCMLoss, CrossEntropyLoss\nfrom libauc.optimizers import PESG, Adam\nfrom libauc.models import densenet121 as DenseNet121\n\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n# from torch.optim import Adam","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.240721Z","iopub.execute_input":"2023-05-07T06:34:01.241096Z","iopub.status.idle":"2023-05-07T06:34:01.254110Z","shell.execute_reply.started":"2023-05-07T06:34:01.241066Z","shell.execute_reply":"2023-05-07T06:34:01.253130Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def set_all_seeds(SEED):\n    # REPRODUCIBILITY\n    torch.manual_seed(SEED)\n    np.random.seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.255825Z","iopub.execute_input":"2023-05-07T06:34:01.256552Z","iopub.status.idle":"2023-05-07T06:34:01.262775Z","shell.execute_reply.started":"2023-05-07T06:34:01.256518Z","shell.execute_reply":"2023-05-07T06:34:01.261989Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"device = torch.device(0 if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.263919Z","iopub.execute_input":"2023-05-07T06:34:01.264538Z","iopub.status.idle":"2023-05-07T06:34:01.334905Z","shell.execute_reply.started":"2023-05-07T06:34:01.264499Z","shell.execute_reply":"2023-05-07T06:34:01.333842Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# m2 = models.video.r3d_18(pretrained=True)\n# m2","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.337380Z","iopub.execute_input":"2023-05-07T06:34:01.337999Z","iopub.status.idle":"2023-05-07T06:34:01.344855Z","shell.execute_reply.started":"2023-05-07T06:34:01.337966Z","shell.execute_reply":"2023-05-07T06:34:01.343869Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# paramaters\nSEED = 123\nBATCH_SIZE = 32\nlr = 3e-4\nweight_decay = 1e-5\nnum_classes=1\n\n# model\nmodel = models.video.r3d_18(pretrained=True)\ninput_channel = model.fc.in_features\n#for param in model.parameters():\n    #param.requires_grad = False\nmodel.fc = nn.Sequential(nn.Linear(input_channel, 128),\n                         nn.ReLU(),\n                         nn.Dropout(p=0.2),\n                         nn.Linear(128, 32),\n                         nn.ReLU(),\n                         nn.Dropout(p=0.1),\n                         nn.Linear(32, num_classes),\n                         nn.Sigmoid()\n                         )\n\n\nmodel = model.to(device)\n\n# create a binary cross-entropy loss function\ncriterion = nn.BCELoss()\noptimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n# training\nbest_val_auc = 0 \nfor epoch in range(10):\n    print(\"epoch: \", epoch+1)\n    for idx, (index, data, labels) in enumerate(train_loader):\n      train_data, train_labels = data, labels\n      train_data=train_data.repeat(1,3,1,1,1)\n      train_labels=train_labels.float()\n\n      train_data, train_labels  = train_data.to(device), train_labels.to(device)\n      # break\n      y_pred = model(train_data.float())\n      loss = criterion(y_pred, train_labels)\n      #print(\"Training Loss= \", loss.item())\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      \n    print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')  \n      # validation  \n      #if idx % 100 == 0:\n      #if (1):\n    model.eval()\n    with torch.no_grad():    \n         test_pred = []\n         test_true = [] \n         for jdx, (index, data, labels) in enumerate(train_loader_at_eval):\n             test_data, test_labels = data, labels\n             test_data=test_data.repeat(1,3,1,1,1)\n             test_labels=test_labels.float()\n             test_data = test_data.to(device)\n             y_pred = model(test_data.float())\n             test_pred.append(y_pred.cpu().detach().numpy())\n             test_true.append(test_labels.numpy())\n     \n         test_true = np.concatenate(test_true)\n         test_pred = np.concatenate(test_pred)\n         val_auc_mean =  roc_auc_score(test_true, test_pred) \n         #model.train\n         if best_val_auc < val_auc_mean:\n            best_val_auc = val_auc_mean\n            torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n         print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc) )","metadata":{"execution":{"iopub.status.busy":"2023-05-07T06:34:01.348017Z","iopub.execute_input":"2023-05-07T06:34:01.348305Z","iopub.status.idle":"2023-05-07T07:06:26.997092Z","shell.execute_reply.started":"2023-05-07T06:34:01.348281Z","shell.execute_reply":"2023-05-07T07:06:26.995740Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n100%|██████████| 127M/127M [00:00<00:00, 257MB/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch:  1\nValidating epoch: 1 Loss: 0.4219638705253601\nEpoch=0, BatchID=73, Val_AUC=0.9495, Best_Val_AUC=0.9495\nepoch:  2\nValidating epoch: 2 Loss: 0.4486767053604126\nEpoch=1, BatchID=73, Val_AUC=0.9088, Best_Val_AUC=0.9495\nepoch:  3\nValidating epoch: 3 Loss: 0.5515609383583069\nEpoch=2, BatchID=73, Val_AUC=0.9187, Best_Val_AUC=0.9495\nepoch:  4\nValidating epoch: 4 Loss: 0.181538924574852\nEpoch=3, BatchID=73, Val_AUC=0.9489, Best_Val_AUC=0.9495\nepoch:  5\nValidating epoch: 5 Loss: 0.22442957758903503\nEpoch=4, BatchID=73, Val_AUC=0.9610, Best_Val_AUC=0.9610\nepoch:  6\nValidating epoch: 6 Loss: 0.12786006927490234\nEpoch=5, BatchID=73, Val_AUC=0.9623, Best_Val_AUC=0.9623\nepoch:  7\nValidating epoch: 7 Loss: 0.06977611035108566\nEpoch=6, BatchID=73, Val_AUC=0.9754, Best_Val_AUC=0.9754\nepoch:  8\nValidating epoch: 8 Loss: 0.17161200940608978\nEpoch=7, BatchID=73, Val_AUC=0.9802, Best_Val_AUC=0.9802\nepoch:  9\nValidating epoch: 9 Loss: 0.09604759514331818\nEpoch=8, BatchID=73, Val_AUC=0.9821, Best_Val_AUC=0.9821\nepoch:  10\nValidating epoch: 10 Loss: 0.01255774311721325\nEpoch=9, BatchID=73, Val_AUC=0.9869, Best_Val_AUC=0.9869\n","output_type":"stream"}]},{"cell_type":"code","source":"    print(f'Testing ...')  \n    PATH = 'ce_pretrained_model.pth' \n    model_state_dict = torch.load(PATH)\n    #model = MyModel()  # Create an instance of your model\n    model.load_state_dict(model_state_dict)  # Load the saved parameters into the model\n    best_val_auc=0.0\n    with torch.no_grad():    \n         test_pred = []\n         test_true = [] \n         for jdx, (index, data, labels) in enumerate(test_loader):\n             test_data, test_labels = data, labels\n             test_data=test_data.repeat(1,3,1,1,1)\n             test_labels=test_labels.float()\n             test_data = test_data.to(device)\n             y_pred = model(test_data.float())\n             test_pred.append(y_pred.cpu().detach().numpy())\n             test_true.append(test_labels.numpy())\n     \n         test_true = np.concatenate(test_true)\n         test_pred = np.concatenate(test_pred)\n         val_auc_mean =  roc_auc_score(test_true, test_pred) \n        # model.train\n        # if best_val_auc < val_auc_mean:\n        #    best_val_auc = val_auc_mean\n        #    #torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n         print('Val_AUC=%.4f'%( val_auc_mean))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:06:26.999411Z","iopub.execute_input":"2023-05-07T07:06:26.999828Z","iopub.status.idle":"2023-05-07T07:06:28.404739Z","shell.execute_reply.started":"2023-05-07T07:06:26.999776Z","shell.execute_reply":"2023-05-07T07:06:28.403504Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Testing ...\nVal_AUC=0.9774\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n# from torch.optim import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# parameters\nclass_id = 1\n\n# paramaters\nSEED = 123\n# BATCH_SIZE = 32 #[16, 32, 64, 128]\n# #imratio = train_dataset.imratio\n# lr = 0.05 # using smaller learning rate is better [0.001, 0.01, 0.05, 0.1]\n\n# lrs = [0.001, 0.01, 0.05, 0.1]\nlrs = [0.1]\n#BATCH_SIZEs = [16, 32, 64, 128]\nBATCH_SIZEs = [32]\n\nepoch_decay = 2e-3\nweight_decay = 1e-5\nmargin = 1.0\n\nfor lr in lrs:\n    print(\"------------------lr: {}--------------------\".format(lr))\n    for BATCH_SIZE in BATCH_SIZEs:\n        print(\"================ Batch Size: {} ===================\".format(BATCH_SIZE))\n        # model\n        set_all_seeds(SEED)\n        # model\n        model = models.video.r3d_18(pretrained=False)\n        input_channel = model.fc.in_features\n        #for param in model.parameters():\n            #param.requires_grad = False\n        model.fc = nn.Sequential(nn.Linear(input_channel, 128),\n                                 nn.ReLU(),\n                                 nn.Dropout(p=0.2),\n                                 nn.Linear(128, 32),\n                                 nn.ReLU(),\n                                 nn.Dropout(p=0.1),\n                                 nn.Linear(32, num_classes),\n                                 nn.Sigmoid()\n                                 )\n\n\n        model = model.to(device)\n\n        #model = models.resnet18(pretrained=False)\n        #\n        #model.fc = nn.Sequential(\n        #    nn.Linear(512, 1),\n        #    nn.Sigmoid()\n        #)\n        #model = model.cuda()\n\n\n        # load pretrained model\n        if True:\n          PATH = 'ce_pretrained_model.pth' \n          state_dict = torch.load(PATH)\n          state_dict.pop('classifier.weight', None)\n          state_dict.pop('classifier.bias', None) \n          model.load_state_dict(state_dict, strict=False)\n\n\n        # define loss & optimizer\n        loss_fn = AUCMLoss()\n        optimizer = PESG(model, \n                         loss_fn=loss_fn, \n                         lr=lr, \n                         margin=margin, \n                         epoch_decay=epoch_decay, \n                         weight_decay=weight_decay)\n        \n#         optimizer = Adam(model.parameters(), lr = lr, weight_decay= weight_decay)\n        lr_scheduler_opt = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1, verbose = True)\n#         scheduler_1 = CosineAnnealingLR(optimizer, 20, eta_min = 1e-08, verbose = True)\n        scheduler_2 = ReduceLROnPlateau(optimizer, patience=2,  verbose=True, factor=0.5, \n                              threshold=1e-04, min_lr=1e-08, mode = 'max')\n\n        best_val_auc = 0\n        for epoch in range(20):\n          for idx, (index, data, labels) in enumerate(train_loader):\n              train_data, train_labels = data, labels\n              train_data=train_data.repeat(1,3,1,1,1)\n              train_labels=train_labels.float()\n\n              train_data, train_labels = train_data.to(device), train_labels.to(device)\n              y_pred = model(train_data.float())\n#               y_pred = torch.sigmoid(y_pred)\n              loss = loss_fn(y_pred, train_labels)\n              optimizer.zero_grad()\n              loss.backward()\n              optimizer.step()\n\n              # validation\n          #if idx % 400 == 0:\n          print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')\n          model.eval()\n          val_auc = 0\n          with torch.no_grad():    \n                test_pred = []\n                test_true = [] \n                for jdx, (index, data, labels) in enumerate(train_loader_at_eval):\n                    test_data, test_label = data, labels\n                    test_data=test_data.repeat(1,3,1,1,1)\n                    test_labels=test_labels.float()\n                    test_data = test_data.to(device)\n                    y_pred = model(test_data.float())\n                    test_pred.append(y_pred.cpu().detach().numpy())\n                    test_true.append(test_label.numpy())\n\n                test_true = np.concatenate(test_true)\n                test_pred = np.concatenate(test_pred)\n                val_auc =  roc_auc_score(test_true, test_pred) \n                #model.train(\n                if best_val_auc < val_auc:\n                   best_val_auc = val_auc\n                   torch.save(model.state_dict(), 'adrenal3d_resnet3d_pesg_0.1_SLR_RLRP.pth'.format(lr, BATCH_SIZE))\n\n          print ('Epoch={}, BatchID={}, Val_AUC={} lr={}'.format(epoch, idx, val_auc, optimizer.lr))\n          scheduler_2.step(val_auc)\n          lr_scheduler_opt.step()\n#           scheduler_1.step()\n        print ('Best Val_AUC is %.4f'%best_val_auc)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-07T07:06:28.406743Z","iopub.execute_input":"2023-05-07T07:06:28.407347Z","iopub.status.idle":"2023-05-07T08:18:45.226360Z","shell.execute_reply.started":"2023-05-07T07:06:28.407307Z","shell.execute_reply":"2023-05-07T08:18:45.224853Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"------------------lr: 0.1--------------------\n================ Batch Size: 32 ===================\nAdjusting learning rate of group 0 to 1.0000e-01.\nValidating epoch: 1 Loss: 0.01638292893767357\nEpoch=0, BatchID=73, Val_AUC=0.9780748663101604 lr=0.1\nAdjusting learning rate of group 0 to 1.0000e-01.\nValidating epoch: 2 Loss: 0.020914414897561073\nEpoch=1, BatchID=73, Val_AUC=0.9866310160427807 lr=0.1\nAdjusting learning rate of group 0 to 1.0000e-01.\nValidating epoch: 3 Loss: 0.03048316389322281\nEpoch=2, BatchID=73, Val_AUC=0.9844919786096257 lr=0.1\nAdjusting learning rate of group 0 to 1.0000e-01.\nValidating epoch: 4 Loss: 0.02076900750398636\nEpoch=3, BatchID=73, Val_AUC=0.9636363636363636 lr=0.1\nAdjusting learning rate of group 0 to 1.0000e-01.\nValidating epoch: 5 Loss: 0.014153522439301014\nEpoch=4, BatchID=73, Val_AUC=0.9802139037433155 lr=0.1\nEpoch 00005: reducing learning rate of group 0 to 5.0000e-02.\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 6 Loss: -0.0001046262914314866\nEpoch=5, BatchID=73, Val_AUC=0.9842245989304813 lr=0.005000000000000001\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 7 Loss: 0.0008309273398481309\nEpoch=6, BatchID=73, Val_AUC=0.9836898395721925 lr=0.005000000000000001\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 8 Loss: -0.00012095441343262792\nEpoch=7, BatchID=73, Val_AUC=0.9834224598930481 lr=0.005000000000000001\nEpoch 00008: reducing learning rate of group 0 to 2.5000e-03.\nAdjusting learning rate of group 0 to 2.5000e-03.\nValidating epoch: 9 Loss: 0.007227911613881588\nEpoch=8, BatchID=73, Val_AUC=0.9834224598930481 lr=0.0025000000000000005\nAdjusting learning rate of group 0 to 2.5000e-03.\nValidating epoch: 10 Loss: 7.925811223685741e-05\nEpoch=9, BatchID=73, Val_AUC=0.9828877005347594 lr=0.0025000000000000005\nAdjusting learning rate of group 0 to 2.5000e-04.\nValidating epoch: 11 Loss: 0.0009546661749482155\nEpoch=10, BatchID=73, Val_AUC=0.9826203208556149 lr=0.00025000000000000006\nEpoch 00011: reducing learning rate of group 0 to 1.2500e-04.\nAdjusting learning rate of group 0 to 1.2500e-04.\nValidating epoch: 12 Loss: 0.0001123174442909658\nEpoch=11, BatchID=73, Val_AUC=0.9826203208556149 lr=0.00012500000000000003\nAdjusting learning rate of group 0 to 1.2500e-04.\nValidating epoch: 13 Loss: 5.440454697236419e-05\nEpoch=12, BatchID=73, Val_AUC=0.9828877005347594 lr=0.00012500000000000003\nAdjusting learning rate of group 0 to 1.2500e-04.\nValidating epoch: 14 Loss: -1.3452197890728712e-05\nEpoch=13, BatchID=73, Val_AUC=0.9836898395721925 lr=0.00012500000000000003\nEpoch 00014: reducing learning rate of group 0 to 6.2500e-05.\nAdjusting learning rate of group 0 to 6.2500e-05.\nValidating epoch: 15 Loss: -8.077436359599233e-05\nEpoch=14, BatchID=73, Val_AUC=0.9836898395721925 lr=6.250000000000001e-05\nAdjusting learning rate of group 0 to 6.2500e-06.\nValidating epoch: 16 Loss: 0.0019516549073159695\nEpoch=15, BatchID=73, Val_AUC=0.9836898395721925 lr=6.250000000000002e-06\nAdjusting learning rate of group 0 to 6.2500e-06.\nValidating epoch: 17 Loss: 0.006420313846319914\nEpoch=16, BatchID=73, Val_AUC=0.9836898395721925 lr=6.250000000000002e-06\nEpoch 00017: reducing learning rate of group 0 to 3.1250e-06.\nAdjusting learning rate of group 0 to 3.1250e-06.\nValidating epoch: 18 Loss: 0.022223882377147675\nEpoch=17, BatchID=73, Val_AUC=0.9836898395721925 lr=3.125000000000001e-06\nAdjusting learning rate of group 0 to 3.1250e-06.\nValidating epoch: 19 Loss: 9.176606545224786e-05\nEpoch=18, BatchID=73, Val_AUC=0.9836898395721925 lr=3.125000000000001e-06\nAdjusting learning rate of group 0 to 3.1250e-06.\nValidating epoch: 20 Loss: 0.0035968187730759382\nEpoch=19, BatchID=73, Val_AUC=0.9836898395721925 lr=3.125000000000001e-06\nEpoch 00020: reducing learning rate of group 0 to 1.5625e-06.\nAdjusting learning rate of group 0 to 1.5625e-07.\nBest Val_AUC is 0.9866\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Testing ...')  \n# PATH = 'aucm_trained_model_0.0003_32_scheduler_20epochs.pth' \nPATH = '/kaggle/working/adrenal3d_resnet3d_pesg_0.1_SLR_RLRP.pth'\nmodel_state_dict = torch.load(PATH)\n#model = MyModel()  # Create an instance of your model\nmodel.load_state_dict(model_state_dict)  # Load the saved parameters into the model\nbest_val_auc=0.0\nwith torch.no_grad():    \n     test_pred = []\n     test_true = [] \n     for jdx, (index, data, labels) in enumerate(test_loader):\n         test_data, test_labels = data, labels\n         test_data=test_data.repeat(1,3,1,1,1)\n         test_labels=test_labels.float()\n         test_data = test_data.to(device)\n         y_pred = model(test_data.float())\n         test_pred.append(y_pred.cpu().detach().numpy())\n         test_true.append(test_labels.numpy())\n \n     test_true = np.concatenate(test_true)\n     test_pred = np.concatenate(test_pred)\n     val_auc_mean =  roc_auc_score(test_true, test_pred) \n    # model.train\n    # if best_val_auc < val_auc_mean:\n    #    best_val_auc = val_auc_mean\n    #    #torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n     print('Val_AUC=%.4f'%( val_auc_mean))","metadata":{"execution":{"iopub.status.busy":"2023-05-07T08:26:08.974216Z","iopub.execute_input":"2023-05-07T08:26:08.974668Z","iopub.status.idle":"2023-05-07T08:26:10.505402Z","shell.execute_reply.started":"2023-05-07T08:26:08.974630Z","shell.execute_reply":"2023-05-07T08:26:10.504293Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Testing ...\nVal_AUC=0.9696\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}