{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":159.576022,"end_time":"2023-05-04T08:22:07.285295","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-04T08:19:27.709273","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.022305,"end_time":"2023-05-04T08:19:40.759125","exception":false,"start_time":"2023-05-04T08:19:40.736820","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:21:39.499562Z","iopub.execute_input":"2023-05-08T23:21:39.500186Z","iopub.status.idle":"2023-05-08T23:21:39.512238Z","shell.execute_reply.started":"2023-05-08T23:21:39.500150Z","shell.execute_reply":"2023-05-08T23:21:39.511326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install libauc==1.2.0\n!pip install medmnist\n!pip install torchio","metadata":{"papermill":{"duration":12.511535,"end_time":"2023-05-04T08:19:53.276351","exception":false,"start_time":"2023-05-04T08:19:40.764816","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:21:40.630605Z","iopub.execute_input":"2023-05-08T23:21:40.630935Z","iopub.status.idle":"2023-05-08T23:22:12.352973Z","shell.execute_reply.started":"2023-05-08T23:21:40.630908Z","shell.execute_reply":"2023-05-08T23:22:12.351480Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport medmnist\nfrom medmnist import INFO, Evaluator\n\nfrom libauc.models import resnet18\nfrom libauc.sampler import DualSampler\nfrom libauc.metrics import auc_prc_score\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageFilter\nfrom torchio import Image","metadata":{"papermill":{"duration":7.988681,"end_time":"2023-05-04T08:20:11","exception":false,"start_time":"2023-05-04T08:20:03.011319","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:12.355118Z","iopub.execute_input":"2023-05-08T23:22:12.355424Z","iopub.status.idle":"2023-05-08T23:22:17.461537Z","shell.execute_reply.started":"2023-05-08T23:22:12.355397Z","shell.execute_reply":"2023-05-08T23:22:17.460603Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")","metadata":{"papermill":{"duration":0.016082,"end_time":"2023-05-04T08:20:11.021699","exception":false,"start_time":"2023-05-04T08:20:11.005617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:17.462879Z","iopub.execute_input":"2023-05-08T23:22:17.463470Z","iopub.status.idle":"2023-05-08T23:22:17.471571Z","shell.execute_reply.started":"2023-05-08T23:22:17.463436Z","shell.execute_reply":"2023-05-08T23:22:17.470515Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"MedMNIST v2.2.1 @ https://github.com/MedMNIST/MedMNIST/\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**CHESTTMNIST DATA**","metadata":{}},{"cell_type":"code","source":"#data_flag = 'breastmnist'\n# data_flag = 'pneumoniamnist'\ndata_flag = 'chestmnist'\n\ndownload = True\n\n#NUM_EPOCHS = 3\n# BATCH_SIZE = 128\n#lr = 0.001\n\ninfo = INFO[data_flag]\ntask = info['task']\nn_channels = info['n_channels']\nn_classes = len(info['label'])\n\nDataClass = getattr(medmnist, info['python_class'])","metadata":{"papermill":{"duration":0.013685,"end_time":"2023-05-04T08:20:11.040985","exception":false,"start_time":"2023-05-04T08:20:11.027300","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:17.474420Z","iopub.execute_input":"2023-05-08T23:22:17.475047Z","iopub.status.idle":"2023-05-08T23:22:17.481846Z","shell.execute_reply.started":"2023-05-08T23:22:17.475014Z","shell.execute_reply":"2023-05-08T23:22:17.480911Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(info)\nprint(n_channels)\nprint(n_classes)\nprint(DataClass)","metadata":{"papermill":{"duration":0.013321,"end_time":"2023-05-04T08:20:11.059655","exception":false,"start_time":"2023-05-04T08:20:11.046334","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:17.483544Z","iopub.execute_input":"2023-05-08T23:22:17.483944Z","iopub.status.idle":"2023-05-08T23:22:17.492674Z","shell.execute_reply.started":"2023-05-08T23:22:17.483911Z","shell.execute_reply":"2023-05-08T23:22:17.491664Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'python_class': 'ChestMNIST', 'description': 'The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.', 'url': 'https://zenodo.org/record/6496656/files/chestmnist.npz?download=1', 'MD5': '02c8a6516a18b556561a56cbdd36c4a8', 'task': 'multi-label, binary-class', 'label': {'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}, 'n_channels': 1, 'n_samples': {'train': 78468, 'val': 11219, 'test': 22433}, 'license': 'CC BY 4.0'}\n1\n14\n<class 'medmnist.dataset.ChestMNIST'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchio.transforms import RandomAffine, RandomFlip, RandomNoise, RandomGamma\nclass ImageDataset(Dataset):\n    def __init__(self, images, targets, image_size=28, crop_size=24, mode='train', kernel_size=3):\n        self.images = images.astype(np.uint8)\n        self.targets = targets\n        self.mode = mode\n\n        self.transform_train = transforms.Compose([\n                                                    transforms.ToTensor(),\n                                                    transforms.Resize((image_size, image_size)),\n                                                    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.2), shear=0.2),\n                                                    transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)], p=0.5),\n                                                    transforms.RandomHorizontalFlip(p=0.5),\n#                                                     transforms.RandomVerticalFlip(p=0.5),\n                                                    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n                                                ])\n\n        self.transform_test = transforms.Compose([\n                             transforms.ToTensor(),\n                             #transforms.GaussianBlur(kernel_size=(kernel_size, kernel_size), sigma=(0.1, 2.0)),\n                             #transforms.BilateralFilter(diameter=5, sigma_color=0.1, sigma_space=15),\n#                              transforms.Resize((image_size, image_size)),\n                              ])\n        \n        \n        # for loss function\n        self.pos_indices = np.flatnonzero(targets==1)\n        self.pos_index_map = {}\n        for i, idx in enumerate(self.pos_indices):\n            self.pos_index_map[idx] = i\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        target = self.targets[idx]\n        image = Image.fromarray(image.astype('uint8'))\n        #image = torch.from_numpy(image).unsqueeze(0) \n        if self.mode == 'train':\n           idx = self.pos_index_map[idx] if idx in self.pos_indices else -1\n           #image = image.filter(ImageFilter.GaussianBlur(radius=3))\n           #image = image.filter(ImageFilter.BLUR)\n           #image = image.filter(ImageFilter.UnsharpMask(radius=3, percent=150, threshold=3))\n\n           image = self.transform_train(image)\n        else:\n           #image = image.filter(ImageFilter.GaussianBlur(radius=3))\n           #image = image.filter(ImageFilter.BLUR)\n           #image = image.filter(ImageFilter.UnsharpMask(radius=3, percent=150, threshold=3))\n           image = self.transform_test(image)\n        return idx, image, target ","metadata":{"papermill":{"duration":0.016686,"end_time":"2023-05-04T08:20:11.081779","exception":false,"start_time":"2023-05-04T08:20:11.065093","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:17.494161Z","iopub.execute_input":"2023-05-08T23:22:17.494734Z","iopub.status.idle":"2023-05-08T23:22:17.506174Z","shell.execute_reply.started":"2023-05-08T23:22:17.494698Z","shell.execute_reply":"2023-05-08T23:22:17.505230Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = DataClass(split='train', download=download)\nprint(train_dataset[0])\nval_dataset = DataClass(split='val', download=download)\nprint(train_dataset[0])\ntest_dataset = DataClass(split='test', download=download)\nprint(test_dataset[0])","metadata":{"papermill":{"duration":4.151358,"end_time":"2023-05-04T08:20:15.238739","exception":false,"start_time":"2023-05-04T08:20:11.087381","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:17.507581Z","iopub.execute_input":"2023-05-08T23:22:17.507984Z","iopub.status.idle":"2023-05-08T23:22:25.377003Z","shell.execute_reply.started":"2023-05-08T23:22:17.507955Z","shell.execute_reply":"2023-05-08T23:22:25.375748Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/record/6496656/files/chestmnist.npz?download=1 to /root/.medmnist/chestmnist.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 82802576/82802576 [00:03<00:00, 20786097.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"(<PIL.Image.Image image mode=L size=28x28 at 0x7D9BC388AA10>, array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\nUsing downloaded and verified file: /root/.medmnist/chestmnist.npz\n(<PIL.Image.Image image mode=L size=28x28 at 0x7D9BC388AA10>, array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\nUsing downloaded and verified file: /root/.medmnist/chestmnist.npz\n(<PIL.Image.Image image mode=L size=28x28 at 0x7D9BC388AA10>, array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n","output_type":"stream"}]},{"cell_type":"code","source":"train_images = np.array([np.asarray(image) for (image, target) in train_dataset])\ntrain_labels = np.array([target for (image, target) in train_dataset])\nprint(type(train_images[0]))","metadata":{"papermill":{"duration":0.057048,"end_time":"2023-05-04T08:20:15.302114","exception":false,"start_time":"2023-05-04T08:20:15.245066","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:47.381273Z","iopub.execute_input":"2023-05-08T23:22:47.381670Z","iopub.status.idle":"2023-05-08T23:22:50.590216Z","shell.execute_reply.started":"2023-05-08T23:22:47.381640Z","shell.execute_reply":"2023-05-08T23:22:50.588454Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"val_images = np.array([np.asarray(image) for (image, target) in val_dataset])\nval_labels = [target for (image, target) in val_dataset]\nprint(type(val_images[0]))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:22:50.592795Z","iopub.execute_input":"2023-05-08T23:22:50.593886Z","iopub.status.idle":"2023-05-08T23:22:51.027230Z","shell.execute_reply.started":"2023-05-08T23:22:50.593849Z","shell.execute_reply":"2023-05-08T23:22:51.026296Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"test_images = np.array([np.asarray(image) for (image, target) in test_dataset])\ntest_labels = [target for (image, target) in test_dataset]\nprint(type(test_images[0]))","metadata":{"papermill":{"duration":0.027115,"end_time":"2023-05-04T08:20:15.336027","exception":false,"start_time":"2023-05-04T08:20:15.308912","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:51.380614Z","iopub.execute_input":"2023-05-08T23:22:51.380966Z","iopub.status.idle":"2023-05-08T23:22:52.346960Z","shell.execute_reply.started":"2023-05-08T23:22:51.380938Z","shell.execute_reply":"2023-05-08T23:22:52.345997Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 512\nsampling_rate = 0.5\n\ntrainSet = ImageDataset(train_images, train_labels)\ntrainSet_eval = ImageDataset(val_images, val_labels, mode='test')\ntestSet = ImageDataset(test_images, test_labels, mode='test')\n\nsampler = DualSampler(trainSet, batch_size, sampling_rate=sampling_rate)\ntrain_loader = torch.utils.data.DataLoader(trainSet, batch_size=batch_size, sampler=sampler, num_workers=2)\ntrain_loader_at_eval = torch.utils.data.DataLoader(trainSet_eval, batch_size=batch_size, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testSet, batch_size=batch_size, shuffle=True, num_workers=2)","metadata":{"papermill":{"duration":0.022577,"end_time":"2023-05-04T08:20:15.365260","exception":false,"start_time":"2023-05-04T08:20:15.342683","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:56.214853Z","iopub.execute_input":"2023-05-08T23:22:56.215185Z","iopub.status.idle":"2023-05-08T23:22:56.319982Z","shell.execute_reply.started":"2023-05-08T23:22:56.215158Z","shell.execute_reply":"2023-05-08T23:22:56.319016Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(train_dataset)\nprint(\"===================\")\nprint(test_dataset)","metadata":{"papermill":{"duration":0.015013,"end_time":"2023-05-04T08:20:15.407395","exception":false,"start_time":"2023-05-04T08:20:15.392382","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:22:57.781221Z","iopub.execute_input":"2023-05-08T23:22:57.781609Z","iopub.status.idle":"2023-05-08T23:22:57.787355Z","shell.execute_reply.started":"2023-05-08T23:22:57.781575Z","shell.execute_reply":"2023-05-08T23:22:57.786264Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Dataset ChestMNIST (chestmnist)\n    Number of datapoints: 78468\n    Root location: /root/.medmnist\n    Split: train\n    Task: multi-label, binary-class\n    Number of channels: 1\n    Meaning of labels: {'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}\n    Number of samples: {'train': 78468, 'val': 11219, 'test': 22433}\n    Description: The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.\n    License: CC BY 4.0\n===================\nDataset ChestMNIST (chestmnist)\n    Number of datapoints: 22433\n    Root location: /root/.medmnist\n    Split: test\n    Task: multi-label, binary-class\n    Number of channels: 1\n    Meaning of labels: {'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}\n    Number of samples: {'train': 78468, 'val': 11219, 'test': 22433}\n    Description: The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.\n    License: CC BY 4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#print(train_dataset[0][0].shape)\n\n# # montage\n# train_dataset.montage(length=20)","metadata":{"papermill":{"duration":0.013591,"end_time":"2023-05-04T08:20:15.427898","exception":false,"start_time":"2023-05-04T08:20:15.414307","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T21:50:06.037908Z","iopub.execute_input":"2023-05-08T21:50:06.038359Z","iopub.status.idle":"2023-05-08T21:50:06.046956Z","shell.execute_reply.started":"2023-05-08T21:50:06.038321Z","shell.execute_reply":"2023-05-08T21:50:06.045950Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Checking Imbalance in Dataset**","metadata":{}},{"cell_type":"code","source":"from libauc.losses import AUCMLoss, CrossEntropyLoss, CompositionalAUCLoss\nfrom libauc.optimizers import PESG, Adam\n# from libauc.models import densenet121 as DenseNet121\n\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR","metadata":{"papermill":{"duration":0.019339,"end_time":"2023-05-04T08:20:15.477390","exception":false,"start_time":"2023-05-04T08:20:15.458051","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-09T00:41:37.562855Z","iopub.execute_input":"2023-05-09T00:41:37.563205Z","iopub.status.idle":"2023-05-09T00:41:37.571508Z","shell.execute_reply.started":"2023-05-09T00:41:37.563176Z","shell.execute_reply":"2023-05-09T00:41:37.570455Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#configure GPU\ndevice = torch.device(0 if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":0.106776,"end_time":"2023-05-04T08:20:15.670537","exception":false,"start_time":"2023-05-04T08:20:15.563761","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-08T23:23:11.365867Z","iopub.execute_input":"2023-05-08T23:23:11.366191Z","iopub.status.idle":"2023-05-08T23:23:11.436923Z","shell.execute_reply.started":"2023-05-08T23:23:11.366164Z","shell.execute_reply":"2023-05-08T23:23:11.435692Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:23:12.455853Z","iopub.execute_input":"2023-05-08T23:23:12.456741Z","iopub.status.idle":"2023-05-08T23:23:12.461918Z","shell.execute_reply.started":"2023-05-08T23:23:12.456705Z","shell.execute_reply":"2023-05-08T23:23:12.460318Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import os, random\ndef set_all_seeds(SEED):\n    # REPRODUCIBILITY\n    torch.manual_seed(SEED)\n    #torch.cuda.manual_seed(SEED)\n    np.random.seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    #os.environ[\"PYTHONHASHSEED\"]=str(SEED)\n    #random.seed(SEED)\n    #torch.use_deterministic_algorithms(True)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:23:13.069394Z","iopub.execute_input":"2023-05-08T23:23:13.070059Z","iopub.status.idle":"2023-05-08T23:23:13.075762Z","shell.execute_reply.started":"2023-05-08T23:23:13.070023Z","shell.execute_reply":"2023-05-08T23:23:13.074798Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"m = models.resnet18()\nm","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:23:16.187987Z","iopub.execute_input":"2023-05-08T23:23:16.188336Z","iopub.status.idle":"2023-05-08T23:23:16.438482Z","shell.execute_reply.started":"2023-05-08T23:23:16.188307Z","shell.execute_reply":"2023-05-08T23:23:16.437577Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# paramaters\nSEED = 123\nBATCH_SIZE = 512\nlr = 3e-4\nweight_decay = 1e-5\nnum_classes=14\n\n# model\nmodel = models.resnet18(pretrained=True)\n# model = models.resnet18(pretrained=False)\n\ninput_channel = model.fc.in_features\n\nmodel.fc = nn.Sequential(nn.Linear(input_channel, 256),\n                         nn.ReLU(),\n#                          nn.Dropout(p=0.2),\n                         nn.Linear(256, 128),\n                         nn.ReLU(),\n#                          nn.Dropout(p=0.1),\n                         nn.Linear(128, num_classes),\n#                          nn.LogSoftmax()\n                        )\n\n\nmodel = model.to(device)\n\n# create a binary cross-entropy loss function\nCELoss = CrossEntropyLoss()\n# NLLLoss = nn.NLLLoss()\noptimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = ReduceLROnPlateau(optimizer, patience=3,  verbose=True, factor=0.5, \n                              threshold=0.0001, min_lr=1e-8, mode = 'max')\n\n# training\nbest_val_auc = 0 \nfor epoch in range(20):\n    print(\"epoch=\",epoch)\n    for idx, (index, data, labels) in enumerate(train_loader):\n      #print(\"idx=\",idx) \n      train_data, train_labels = data.to(device), labels.to(device)\n      train_data=train_data.repeat(1,3,1,1)\n      y_pred = model(train_data)\n#       print(y_pred)\n#       print(train_labels.float())\n      loss = CELoss(y_pred, train_labels.float())\n      #print(\"Training Loss= \", loss.item())\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      \n    print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')  \n    model.eval()\n    val_auc_mean = 0\n\n    with torch.no_grad():    \n         val_pred = []\n         val_true = [] \n         for jdx, (index, data, targets) in enumerate(train_loader_at_eval):\n             val_data, val_labels = data.to(device), targets.to(device)\n             val_data=val_data.repeat(1,3,1,1)\n\n             y_pred = model(val_data)\n             val_pred.append(y_pred.cpu().detach().numpy())\n             val_true.append(val_labels.cpu().detach().numpy())\n     \n         val_true = np.concatenate(val_true)\n         val_pred = np.concatenate(val_pred)\n         #val_pred = [1 if x > 0.5 else 0 for x in val_pred]\n         val_auc_mean =  roc_auc_score(val_true, val_pred) \n         \n         if best_val_auc < val_auc_mean:\n            best_val_auc = val_auc_mean\n            torch.save(model.state_dict(), 'ce_bmnist_pretrained_model.pth')\n         print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc) )\n    scheduler.step(val_auc_mean)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T23:45:07.069689Z","iopub.execute_input":"2023-05-08T23:45:07.070062Z","iopub.status.idle":"2023-05-09T00:27:49.793899Z","shell.execute_reply.started":"2023-05-08T23:45:07.070026Z","shell.execute_reply":"2023-05-09T00:27:49.792642Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"epoch= 0\nValidating epoch: 1 Loss: 0.22213214635849\nEpoch=0, BatchID=274, Val_AUC=0.7004, Best_Val_AUC=0.7004\nepoch= 1\nValidating epoch: 2 Loss: 0.21465912461280823\nEpoch=1, BatchID=274, Val_AUC=0.7074, Best_Val_AUC=0.7074\nepoch= 2\nValidating epoch: 3 Loss: 0.2101481705904007\nEpoch=2, BatchID=274, Val_AUC=0.7205, Best_Val_AUC=0.7205\nepoch= 3\nValidating epoch: 4 Loss: 0.2197813093662262\nEpoch=3, BatchID=274, Val_AUC=0.7272, Best_Val_AUC=0.7272\nepoch= 4\nValidating epoch: 5 Loss: 0.20240944623947144\nEpoch=4, BatchID=274, Val_AUC=0.7352, Best_Val_AUC=0.7352\nepoch= 5\nValidating epoch: 6 Loss: 0.2080138474702835\nEpoch=5, BatchID=274, Val_AUC=0.7322, Best_Val_AUC=0.7352\nepoch= 6\nValidating epoch: 7 Loss: 0.20323942601680756\nEpoch=6, BatchID=274, Val_AUC=0.7383, Best_Val_AUC=0.7383\nepoch= 7\nValidating epoch: 8 Loss: 0.19559107720851898\nEpoch=7, BatchID=274, Val_AUC=0.7293, Best_Val_AUC=0.7383\nepoch= 8\nValidating epoch: 9 Loss: 0.20457823574543\nEpoch=8, BatchID=274, Val_AUC=0.7384, Best_Val_AUC=0.7384\nepoch= 9\nValidating epoch: 10 Loss: 0.1978965848684311\nEpoch=9, BatchID=274, Val_AUC=0.7341, Best_Val_AUC=0.7384\nepoch= 10\nValidating epoch: 11 Loss: 0.20982740819454193\nEpoch=10, BatchID=274, Val_AUC=0.7236, Best_Val_AUC=0.7384\nepoch= 11\nValidating epoch: 12 Loss: 0.1979309618473053\nEpoch=11, BatchID=274, Val_AUC=0.7231, Best_Val_AUC=0.7384\nepoch= 12\nValidating epoch: 13 Loss: 0.19604335725307465\nEpoch=12, BatchID=274, Val_AUC=0.7277, Best_Val_AUC=0.7384\nEpoch 00013: reducing learning rate of group 0 to 1.5000e-04.\nepoch= 13\nValidating epoch: 14 Loss: 0.18903936445713043\nEpoch=13, BatchID=274, Val_AUC=0.7169, Best_Val_AUC=0.7384\nepoch= 14\nValidating epoch: 15 Loss: 0.190948024392128\nEpoch=14, BatchID=274, Val_AUC=0.7154, Best_Val_AUC=0.7384\nepoch= 15\nValidating epoch: 16 Loss: 0.17299237847328186\nEpoch=15, BatchID=274, Val_AUC=0.7143, Best_Val_AUC=0.7384\nepoch= 16\nValidating epoch: 17 Loss: 0.1590898036956787\nEpoch=16, BatchID=274, Val_AUC=0.6934, Best_Val_AUC=0.7384\nEpoch 00017: reducing learning rate of group 0 to 7.5000e-05.\nepoch= 17\nValidating epoch: 18 Loss: 0.1587444394826889\nEpoch=17, BatchID=274, Val_AUC=0.6987, Best_Val_AUC=0.7384\nepoch= 18\nValidating epoch: 19 Loss: 0.1624935418367386\nEpoch=18, BatchID=274, Val_AUC=0.6901, Best_Val_AUC=0.7384\nepoch= 19\nValidating epoch: 20 Loss: 0.14467471837997437\nEpoch=19, BatchID=274, Val_AUC=0.6955, Best_Val_AUC=0.7384\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Pretraining2","metadata":{}},{"cell_type":"code","source":"# # paramaters\n# SEED = 123\n# BATCH_SIZE = 32\n# lr = 1e-4\n# weight_decay = 1e-5\n# num_classes=14\n\n# model = models.resnet18(pretrained=True)\n# input_channel = model.fc.in_features\n# #for param in model.parameters():\n#     #param.requires_grad = False\n# model.fc = nn.Sequential(nn.Linear(input_channel, 256),\n#                          nn.ReLU(),\n#                          nn.Dropout(p=0.2),\n#                          nn.Linear(256, 128),\n#                          nn.ReLU(),\n#                          nn.Dropout(p=0.1),\n#                          nn.Linear(128, num_classes)\n# #                          nn.LogSoftmax()\n#                         )\n\n\n# model = model.to(device)\n\n# #model = models.resnet18(pretrained=False)\n# #\n# #model.fc = nn.Sequential(\n# #    nn.Linear(512, 1),\n# #    nn.Sigmoid()\n# #)\n# #model = model.cuda()\n\n\n# # load pretrained model\n# if True:\n#   PATH = '/kaggle/input/chest-model/ce_cmnist_pretrained_model.pth' \n#   state_dict = torch.load(PATH)\n#   #state_dict.pop('classifier.weight', None)\n#   #state_dict.pop('classifier.bias', None) \n#   model.load_state_dict(state_dict, strict=False)\n\n\n# # create a binary cross-entropy loss function\n# CELoss = CrossEntropyLoss()\n# optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n# scheduler = ReduceLROnPlateau(optimizer, patience=3,  verbose=True, factor=0.5, \n#                               threshold=0.0001, min_lr=1e-8, mode = 'max')\n\n# # training\n# best_val_auc = 0 \n# for epoch in range(20):\n#     print(\"epoch=\",epoch)\n#     for idx, (index, data, labels) in enumerate(train_loader):\n#       #print(\"idx=\",idx) \n#       train_data, train_labels = data.to(device), labels.to(device)\n#       train_data=train_data.repeat(1,3,1,1)\n#       y_pred = model(train_data)\n#       loss = CELoss(y_pred, train_labels.float())\n#       #print(\"Training Loss= \", loss.item())\n#       optimizer.zero_grad()\n#       loss.backward()\n#       optimizer.step()\n\n      \n#     print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')  \n#     model.eval()\n#     val_auc_mean = 0\n\n#     with torch.no_grad():    \n#          val_pred = []\n#          val_true = [] \n#          for jdx, (index, data, targets) in enumerate(train_loader_at_eval):\n#              val_data, val_labels = data.to(device), targets.to(device)\n#              val_data=val_data.repeat(1,3,1,1)\n\n#              y_pred = model(val_data)\n#              val_pred.append(y_pred.cpu().detach().numpy())\n#              val_true.append(val_labels.cpu().detach().numpy())\n     \n#          val_true = np.concatenate(val_true)\n#          val_pred = np.concatenate(val_pred)\n#          #val_pred = [1 if x > 0.5 else 0 for x in val_pred]\n#          val_auc_mean =  roc_auc_score(val_true, val_pred) \n         \n#          if best_val_auc < val_auc_mean:\n#             best_val_auc = val_auc_mean\n#             torch.save(model.state_dict(), 'ce_cmnist_pretrained_model_2.pth')\n#          print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc) )\n#     scheduler.step(val_auc_mean)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:36:37.553068Z","iopub.execute_input":"2023-05-08T18:36:37.553525Z","iopub.status.idle":"2023-05-08T19:24:17.176079Z","shell.execute_reply.started":"2023-05-08T18:36:37.553486Z","shell.execute_reply":"2023-05-08T19:24:17.174972Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"epoch= 0\nValidating epoch: 1 Loss: 0.21057210862636566\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=0, BatchID=274, Val_AUC=0.7514, Best_Val_AUC=0.7514\nepoch= 1\nValidating epoch: 2 Loss: 0.20706136524677277\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=1, BatchID=274, Val_AUC=0.7663, Best_Val_AUC=0.7663\nepoch= 2\nValidating epoch: 3 Loss: 0.18850626051425934\nEpoch=2, BatchID=274, Val_AUC=0.7653, Best_Val_AUC=0.7663\nepoch= 3\nValidating epoch: 4 Loss: 0.18569399416446686\nEpoch=3, BatchID=274, Val_AUC=0.7722, Best_Val_AUC=0.7722\nepoch= 4\nValidating epoch: 5 Loss: 0.18701043725013733\nEpoch=4, BatchID=274, Val_AUC=0.7742, Best_Val_AUC=0.7742\nepoch= 5\nValidating epoch: 6 Loss: 0.18936726450920105\nEpoch=5, BatchID=274, Val_AUC=0.7822, Best_Val_AUC=0.7822\nepoch= 6\nValidating epoch: 7 Loss: 0.17394167184829712\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=6, BatchID=274, Val_AUC=0.7851, Best_Val_AUC=0.7851\nepoch= 7\nValidating epoch: 8 Loss: 0.18493129312992096\nEpoch=7, BatchID=274, Val_AUC=0.7842, Best_Val_AUC=0.7851\nepoch= 8\nValidating epoch: 9 Loss: 0.17420141398906708\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=8, BatchID=274, Val_AUC=0.7951, Best_Val_AUC=0.7951\nepoch= 9\nValidating epoch: 10 Loss: 0.17208394408226013\nEpoch=9, BatchID=274, Val_AUC=0.8000, Best_Val_AUC=0.8000\nepoch= 10\nValidating epoch: 11 Loss: 0.1713232696056366\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=10, BatchID=274, Val_AUC=0.7990, Best_Val_AUC=0.8000\nepoch= 11\nValidating epoch: 12 Loss: 0.16280314326286316\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=11, BatchID=274, Val_AUC=0.8047, Best_Val_AUC=0.8047\nepoch= 12\nValidating epoch: 13 Loss: 0.15564578771591187\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=12, BatchID=274, Val_AUC=0.8046, Best_Val_AUC=0.8047\nepoch= 13\nValidating epoch: 14 Loss: 0.15669678151607513\nEpoch=13, BatchID=274, Val_AUC=0.8110, Best_Val_AUC=0.8110\nepoch= 14\nValidating epoch: 15 Loss: 0.1546756774187088\nEpoch=14, BatchID=274, Val_AUC=0.8141, Best_Val_AUC=0.8141\nepoch= 15\nValidating epoch: 16 Loss: 0.14047561585903168\nEpoch=15, BatchID=274, Val_AUC=0.8257, Best_Val_AUC=0.8257\nepoch= 16\nValidating epoch: 17 Loss: 0.13880035281181335\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=16, BatchID=274, Val_AUC=0.8252, Best_Val_AUC=0.8257\nepoch= 17\nValidating epoch: 18 Loss: 0.13368859887123108\nEpoch=17, BatchID=274, Val_AUC=0.8280, Best_Val_AUC=0.8280\nepoch= 18\nValidating epoch: 19 Loss: 0.12919074296951294\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=18, BatchID=274, Val_AUC=0.8284, Best_Val_AUC=0.8284\nepoch= 19\nValidating epoch: 20 Loss: 0.1380927413702011\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7caf430ce3b0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n    if w.is_alive():\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=19, BatchID=274, Val_AUC=0.8335, Best_Val_AUC=0.8335\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T19:24:22.234778Z","iopub.execute_input":"2023-05-08T19:24:22.235169Z","iopub.status.idle":"2023-05-08T19:24:22.241427Z","shell.execute_reply.started":"2023-05-08T19:24:22.235130Z","shell.execute_reply":"2023-05-08T19:24:22.240296Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=128, out_features=32, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.1, inplace=False)\n    (6): Linear(in_features=32, out_features=14, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    print(f'Testing ...')  \n    PATH = '/kaggle/working/ce_bmnist_pretrained_model.pth' \n    model_state_dict = torch.load(PATH)\n    #model = MyModel()  # Create an instance of your model\n    model.load_state_dict(model_state_dict)  # Load the saved parameters into the model\n    with torch.no_grad():    \n         test_pred = []\n         test_true = [] \n         for jdx, (index, data, targets) in enumerate(test_loader):\n             test_data, test_labels = data.to(device), targets.to(device)\n             test_data=test_data.repeat(1,3,1,1)\n#              test_labels=test_labels.float()\n#              test_data = test_data.to(device)\n             y_pred = model(test_data)\n             test_pred.append(y_pred.cpu().detach().numpy())\n             test_true.append(test_labels.cpu().detach().numpy())\n     \n         test_true = np.concatenate(test_true)\n         test_pred = np.concatenate(test_pred)\n         #test_pred = [1 if x > 0.5 else 0 for x in test_pred]\n         test_auc_mean =  roc_auc_score(test_true, test_pred) \n        # model.train\n        # if best_val_auc < val_auc_mean:\n        #    best_val_auc = val_auc_mean\n        #    #torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n         print('Test_AUC=%.4f'%( test_auc_mean))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T00:40:29.983581Z","iopub.execute_input":"2023-05-09T00:40:29.984010Z","iopub.status.idle":"2023-05-09T00:40:32.673732Z","shell.execute_reply.started":"2023-05-09T00:40:29.983980Z","shell.execute_reply":"2023-05-09T00:40:32.672471Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Testing ...\nTest_AUC=0.7342\n","output_type":"stream"}]},{"cell_type":"code","source":"from ignite.engine import *\nfrom ignite.handlers import *\nfrom ignite.metrics import *\nfrom ignite.utils import *\nfrom ignite.contrib.metrics.regression import *\nfrom ignite.contrib.metrics import *\nfrom torch.optim.lr_scheduler import ExponentialLR\n\n\n# parameters\nclass_id = 1\nnum_classes=14\n# paramaters\nSEED = 123\nBATCH_SIZE = 512 #[16, 32, 64, 128]\n#imratio = train_dataset.imratio\nlr =0.05#0.05# 0.04#0.05 # using smaller learning rate is better [0.001, 0.01, 0.05, 0.1]\nepoch_decay = 2e-3#1e-4\nweight_decay = 1e-5#1e-5\nmargin = 1.0\n\n# model\nset_all_seeds(SEED)\n# model\nmodel = models.resnet18(pretrained=False)\ninput_channel = model.fc.in_features\n#for param in model.parameters():\n    #param.requires_grad = False\nmodel.fc = nn.Sequential(nn.Linear(input_channel, 256),\n                         nn.ReLU(),\n#                          nn.Dropout(p=0.2),\n                         nn.Linear(256, 128),\n                         nn.ReLU(),\n#                          nn.Dropout(p=0.1),\n                         nn.Linear(128, num_classes),\n#                          nn.LogSoftmax()\n                        )\n\nmodel = model.to(device)\n\n#model = models.resnet18(pretrained=False)\n#\n#model.fc = nn.Sequential(\n#    nn.Linear(512, 1),\n#    nn.Sigmoid()\n#)\n#model = model.cuda()\n\n\n# load pretrained model\nif True:\n  PATH = '/kaggle/working/ce_bmnist_pretrained_model.pth' \n  state_dict = torch.load(PATH)\n  state_dict.pop('classifier.weight', None)\n  state_dict.pop('classifier.bias', None) \n  model.load_state_dict(state_dict, strict=False)\n\n\n# define loss & optimizer\n# loss_fn = AUCMLoss()\nloss_fn = CompositionalAUCLoss()\noptimizer = PESG(model,\n                 loss_fn=loss_fn, \n                 lr=lr, \n                 margin=margin, \n                 epoch_decay=epoch_decay, \n                 weight_decay=weight_decay)\n#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n#lr_scheduler_opt = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n#optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = ReduceLROnPlateau(optimizer, patience=3,  verbose=True, factor=0.5, \n                              threshold=0.0001, min_lr=1e-8, mode = 'max')\n\nbest_val_auc = 0\nfor epoch in range(20):\n  print(\"epoch=\",epoch)\n  if epoch%10 == 0 and epoch > 0:\n      optimizer.update_regularizer(decay_factor=10)\n  #lr_scheduler_opt.step()\n  \n  for idx, (index, data, labels) in enumerate(train_loader):\n      train_data, train_labels = data.to(device), labels.to(device)\n      train_data=train_data.repeat(1,3,1,1)\n#       train_labels=train_labels.float()\n\n#       train_data, train_labels = train_data.to(device), train_labels.to(device)\n      y_pred = model(train_data)\n      #y_pred = torch.sigmoid(y_pred)\n      #y_pred=torch.where(y_pred > 0.5, torch.tensor([1.], device=device), torch.tensor([0.], device=device))\n      #y_pred = [1 if x > 0.5 else 0 for x in y_pred]\n      #print(y_pred)\n      loss = loss_fn(y_pred, train_labels.float())\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      # validation\n  #if idx % 400 == 0:\n  print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')\n  model.eval()\n  val_auc = 0\n  with torch.no_grad():    \n        test_pred = []\n        test_true = [] \n        for jdx, (index, data, targets) in enumerate(train_loader_at_eval):\n            test_data, test_label = data.to(device), targets.to(device)\n            test_data=test_data.repeat(1,3,1,1)\n#             test_labels=test_labels.float()\n#             test_data = test_data.to(device)\n            y_pred = model(test_data)\n            test_pred.append(y_pred.cpu().detach().numpy())\n            test_true.append(test_label.cpu().detach().numpy())\n        \n        test_true = np.concatenate(test_true)\n        test_pred = np.concatenate(test_pred)\n        #test_pred = [1 if x > 0.5 else 0 for x in test_pred]\n        val_auc =  roc_auc_score(test_true, test_pred) \n        #model.train(\n        if best_val_auc < val_auc:\n           best_val_auc = val_auc\n        #if epoch==99:\n           torch.save(model, 'aucm_trained_model_chestmnist.pth')\n  scheduler.step(val_auc)\n  lr_scheduler.step()\n  #scheduler.step()\n  print ('Epoch=%s, BatchID=%s, Val_AUC=%.7f, lr=%.15f'%(epoch, idx, val_auc,  optimizer.lr))\n  #for param_group in optimizer.param_groups:\n   #     print('Epoch {}, LR: {}'.format(epoch, param_group['lr']))\nprint ('Best Val_AUC is %.4f'%best_val_auc)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-09T00:41:46.670328Z","iopub.execute_input":"2023-05-09T00:41:46.670709Z","iopub.status.idle":"2023-05-09T01:24:26.382793Z","shell.execute_reply.started":"2023-05-09T00:41:46.670681Z","shell.execute_reply":"2023-05-09T01:24:26.381514Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"epoch= 0\nValidating epoch: 1 Loss: 0.24280396103858948\nEpoch=0, BatchID=274, Val_AUC=0.7183754, lr=0.050000000000000\nepoch= 1\nValidating epoch: 2 Loss: -0.23712235689163208\nEpoch=1, BatchID=274, Val_AUC=0.7300261, lr=0.050000000000000\nepoch= 2\nValidating epoch: 3 Loss: 0.2758781313896179\nEpoch=2, BatchID=274, Val_AUC=0.7330958, lr=0.050000000000000\nepoch= 3\nValidating epoch: 4 Loss: -0.11821025609970093\nEpoch=3, BatchID=274, Val_AUC=0.7329857, lr=0.050000000000000\nepoch= 4\nValidating epoch: 5 Loss: 0.29243895411491394\nEpoch=4, BatchID=274, Val_AUC=0.7318068, lr=0.050000000000000\nepoch= 5\nValidating epoch: 6 Loss: 0.5657498836517334\nEpoch=5, BatchID=274, Val_AUC=0.7307633, lr=0.050000000000000\nepoch= 6\nValidating epoch: 7 Loss: 0.37884604930877686\nEpoch 00007: reducing learning rate of group 0 to 2.5000e-02.\nEpoch=6, BatchID=274, Val_AUC=0.7256421, lr=0.050000000000000\nepoch= 7\nValidating epoch: 8 Loss: 2.535165309906006\nEpoch=7, BatchID=274, Val_AUC=0.7252152, lr=0.025000000000000\nepoch= 8\nValidating epoch: 9 Loss: 0.34719905257225037\nEpoch=8, BatchID=274, Val_AUC=0.7240578, lr=0.025000000000000\nepoch= 9\nValidating epoch: 10 Loss: 4.424914836883545\nEpoch=9, BatchID=274, Val_AUC=0.7217468, lr=0.025000000000000\nepoch= 10\nReducing learning rate to 0.00025 @ T=2750!\nUpdating regularizer @ T=2750!\nValidating epoch: 11 Loss: 0.35933011770248413\nEpoch 00011: reducing learning rate of group 0 to 1.2500e-04.\nEpoch=10, BatchID=274, Val_AUC=0.7225857, lr=0.000250000000000\nepoch= 11\nValidating epoch: 12 Loss: 7.191028118133545\nEpoch=11, BatchID=274, Val_AUC=0.7228670, lr=0.000125000000000\nepoch= 12\nValidating epoch: 13 Loss: 0.3573136627674103\nEpoch=12, BatchID=274, Val_AUC=0.7230880, lr=0.000125000000000\nepoch= 13\nValidating epoch: 14 Loss: 5.226830005645752\nEpoch=13, BatchID=274, Val_AUC=0.7232188, lr=0.000125000000000\nepoch= 14\nValidating epoch: 15 Loss: 0.34906232357025146\nEpoch 00015: reducing learning rate of group 0 to 6.2500e-05.\nEpoch=14, BatchID=274, Val_AUC=0.7233028, lr=0.000125000000000\nepoch= 15\nValidating epoch: 16 Loss: 4.997647762298584\nEpoch=15, BatchID=274, Val_AUC=0.7233261, lr=0.000062500000000\nepoch= 16\nValidating epoch: 17 Loss: 0.34451231360435486\nEpoch=16, BatchID=274, Val_AUC=0.7233680, lr=0.000062500000000\nepoch= 17\nValidating epoch: 18 Loss: 5.838987350463867\nEpoch=17, BatchID=274, Val_AUC=0.7233894, lr=0.000062500000000\nepoch= 18\nValidating epoch: 19 Loss: 0.3399752378463745\nEpoch 00019: reducing learning rate of group 0 to 3.1250e-05.\nEpoch=18, BatchID=274, Val_AUC=0.7234184, lr=0.000062500000000\nepoch= 19\nValidating epoch: 20 Loss: 4.694250106811523\nEpoch=19, BatchID=274, Val_AUC=0.7234248, lr=0.000031250000000\nBest Val_AUC is 0.7331\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Testing ...')  \nPATH = '/kaggle/working/aucm_trained_model_chestmnist.pth' \nmodel = torch.load(PATH)\n#model = MyModel()  # Create an instance of your model\n##model=torch.load(model_dumped)  # Load the saved parameters into the model\nbest_val_auc=0.0\nwith torch.no_grad():    \n     test_pred = []\n     test_true = [] \n     for jdx, (index, data, targets) in enumerate(test_loader):\n         test_data, test_labels = data.to(device), targets.to(device)\n         test_data=test_data.repeat(1,3,1,1)\n#          test_labels=test_labels.float()\n#          test_data = test_data.to(device)\n         y_pred = model(test_data)\n         test_pred.append(y_pred.cpu().detach().numpy())\n         test_true.append(test_labels.cpu().detach().numpy())\n \n     test_true = np.concatenate(test_true)\n     test_pred = np.concatenate(test_pred)\n     #test_pred = [1 if x > 0.5 else 0 for x in test_pred]\n     #print(test_pred)\n     #print(test_true)\n     val_auc_mean =  roc_auc_score((test_true), test_pred) \n    # model.train\n    # if best_val_auc < val_auc_mean:\n    #    best_val_auc = val_auc_mean\n    #    #torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n     print('Test_AUC=%.7f'%( val_auc_mean))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T01:26:30.580487Z","iopub.execute_input":"2023-05-09T01:26:30.581269Z","iopub.status.idle":"2023-05-09T01:26:33.268015Z","shell.execute_reply.started":"2023-05-09T01:26:30.581231Z","shell.execute_reply":"2023-05-09T01:26:33.266617Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Testing ...\nTest_AUC=0.7274890\n","output_type":"stream"}]}]}