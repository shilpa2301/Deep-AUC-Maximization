{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-08T01:02:07.019442Z","iopub.execute_input":"2023-05-08T01:02:07.019797Z","iopub.status.idle":"2023-05-08T01:02:07.029807Z","shell.execute_reply.started":"2023-05-08T01:02:07.019767Z","shell.execute_reply":"2023-05-08T01:02:07.028897Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install libauc==1.2.0\n!pip install medmnist\n!pip install torchio","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:02:07.631488Z","iopub.execute_input":"2023-05-08T01:02:07.632451Z","iopub.status.idle":"2023-05-08T01:02:38.487867Z","shell.execute_reply.started":"2023-05-08T01:02:07.632408Z","shell.execute_reply":"2023-05-08T01:02:38.486622Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport medmnist\nfrom medmnist import INFO, Evaluator\n\nfrom libauc.models import resnet18\nfrom libauc.sampler import DualSampler\nfrom libauc.metrics import auc_prc_score\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nfrom torchio import Image","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:02:38.491050Z","iopub.execute_input":"2023-05-08T01:02:38.492467Z","iopub.status.idle":"2023-05-08T01:02:43.414135Z","shell.execute_reply.started":"2023-05-08T01:02:38.492421Z","shell.execute_reply":"2023-05-08T01:02:43.413268Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:02:43.415336Z","iopub.execute_input":"2023-05-08T01:02:43.415919Z","iopub.status.idle":"2023-05-08T01:02:43.423111Z","shell.execute_reply.started":"2023-05-08T01:02:43.415886Z","shell.execute_reply":"2023-05-08T01:02:43.422192Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"MedMNIST v2.2.1 @ https://github.com/MedMNIST/MedMNIST/\n","output_type":"stream"}]},{"cell_type":"code","source":"#data_flag = 'breastmnist'\n#data_flag = 'pneumoniamnist'\n#data_flag = 'chestmnist'\n#data_flag = 'nodulemnist3d'\n#data_flag = 'adrenalmnist3d'\ndata_flag = 'synapsemnist3d'\n\ndownload = True\n\n#NUM_EPOCHS = 3\nBATCH_SIZE = 32\n#lr = 0.001\n\ninfo = INFO[data_flag]\ntask = info['task']\nn_channels = info['n_channels']\nn_classes = len(info['label'])\n\nDataClass = getattr(medmnist, info['python_class'])\n\nprint(info)\nprint(n_channels)\nprint(n_classes)\nprint(DataClass)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:02:43.425592Z","iopub.execute_input":"2023-05-08T01:02:43.426233Z","iopub.status.idle":"2023-05-08T01:02:43.435007Z","shell.execute_reply.started":"2023-05-08T01:02:43.426169Z","shell.execute_reply":"2023-05-08T01:02:43.434045Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'python_class': 'SynapseMNIST3D', 'description': 'The SynapseMNIST3D is a new 3D volume dataset to classify whether a synapse is excitatory or inhibitory. It uses a 3D image volume of an adult rat acquired by a multi-beam scanning electron microscope. The original data is of the size 100×100×100um^3 and the resolution 8×8×30nm^3, where a (30um)^3 sub-volume was used in the MitoEM dataset with dense 3D mitochondria instance segmentation labels. Three neuroscience experts segment a pyramidal neuron within the whole volume and proofread all the synapses on this neuron with excitatory/inhibitory labels. For each labeled synaptic location, we crop a 3D volume of 1024×1024×1024nm^3 and resize it into 28×28×28 voxels. Finally, the dataset is randomly split with a ratio of 7:1:2 into training, validation and test set.', 'url': 'https://zenodo.org/record/6496656/files/synapsemnist3d.npz?download=1', 'MD5': '1235b78a3cd6280881dd7850a78eadb6', 'task': 'binary-class', 'label': {'0': 'inhibitory synapse', '1': 'excitatory synapse'}, 'n_channels': 1, 'n_samples': {'train': 1230, 'val': 177, 'test': 352}, 'license': 'CC BY 4.0'}\n1\n2\n<class 'medmnist.dataset.SynapseMNIST3D'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchio.transforms import RandomAffine, RandomFlip, RandomNoise, RandomGamma\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, images, targets, image_size=28, crop_size=24, mode='train'):\n        self.images = images\n        self.targets = targets\n        self.mode = mode\n        \n        self.transform_train = transforms.Compose([RandomAffine(scales=(0.9, 1.2),\n                                                                degrees=15,\n                                                                p=0.5),\n#                                                    RandomAffine(),\n                                                   RandomGamma(log_gamma=(-0.3, 0.3), p=0.5),\n                                                   RandomFlip(p = 0.5),\n                                                   RandomNoise(p=0.2),\n                                                  ])\n        self.transform_test = transforms.Compose([RandomAffine(p=0),\n#                                                    RandomAffine(),\n                                                   RandomFlip(p=0),\n                                                   RandomNoise(p=0),\n                                                  ])\n        \n#         increased_dataset = torch.utils.data.ConcatDataset([transformed_dataset,original])\n        \n#         self.transform_train = transforms.Compose([                                                \n#                               transforms.ToTensor(),\n# #                               transforms.RandomCrop((crop_size, crop_size, crop_size), padding=None),\n#                               transforms.RandomHorizontalFlip(),\n# #                               transforms.Resize((image_size, image_size, image_size)),\n#                               ])\n#         self.transform_test = transforms.Compose([\n#                              transforms.ToTensor(),\n# #                              transforms.Resize((image_size, image_size, image_size)),\n# #                              transforms.Normalize(mean=[.5], std=[.5])\n#                               ])\n        \n        \n        # for loss function\n        self.pos_indices = np.flatnonzero(targets==1)\n        self.pos_index_map = {}\n        for i, idx in enumerate(self.pos_indices):\n            self.pos_index_map[idx] = i\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        target = self.targets[idx]\n#         image = Image.fromarray(image.astype('uint8'))\n        if self.mode == 'train':\n           idx = self.pos_index_map[idx] if idx in self.pos_indices else -1\n           image = self.transform_train(image)\n        else:\n           image = self.transform_test(image)\n        return idx, image, target ","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:02:43.436611Z","iopub.execute_input":"2023-05-08T01:02:43.436979Z","iopub.status.idle":"2023-05-08T01:02:43.452650Z","shell.execute_reply.started":"2023-05-08T01:02:43.436948Z","shell.execute_reply":"2023-05-08T01:02:43.451735Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = DataClass(split='train',  download=download)\n\n# encapsulate data into dataloader form\n#train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nval_dataset = DataClass(split='val',  download=download)\n#train_loader_at_eval = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = DataClass(split='test',  download=download)\n#test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:02:43.453916Z","iopub.execute_input":"2023-05-08T01:02:43.454266Z","iopub.status.idle":"2023-05-08T01:03:20.366468Z","shell.execute_reply.started":"2023-05-08T01:02:43.454220Z","shell.execute_reply":"2023-05-08T01:03:20.365476Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/record/6496656/files/synapsemnist3d.npz?download=1 to /root/.medmnist/synapsemnist3d.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38034583/38034583 [00:33<00:00, 1149121.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using downloaded and verified file: /root/.medmnist/synapsemnist3d.npz\nUsing downloaded and verified file: /root/.medmnist/synapsemnist3d.npz\n","output_type":"stream"}]},{"cell_type":"code","source":"train_images = np.array([np.asarray(image) for (image, target) in train_dataset])\ntrain_labels = np.array([target for (image, target) in train_dataset])\nprint(type(train_images[0]))\n\nval_images = np.array([np.asarray(image) for (image, target) in val_dataset])\nval_labels = np.array([target for (image, target) in val_dataset])\nprint(type(val_images[0]))\n\ntest_images = np.array([np.asarray(image) for (image, target) in test_dataset])\ntest_labels = [target for (image, target) in test_dataset]\nprint(type(test_images[0]))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:20.367921Z","iopub.execute_input":"2023-05-08T01:03:20.368250Z","iopub.status.idle":"2023-05-08T01:03:21.021799Z","shell.execute_reply.started":"2023-05-08T01:03:20.368202Z","shell.execute_reply":"2023-05-08T01:03:21.020711Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 16\nsampling_rate = 0.5\n\ntrainSet = ImageDataset(train_images, train_labels)\ntrainSet_eval = ImageDataset(val_images, val_labels, mode='test')\ntestSet = ImageDataset(test_images, test_labels, mode='test')\n\nsampler = DualSampler(dataset=trainSet, batch_size=batch_size, sampling_rate=sampling_rate)\ntrain_loader = torch.utils.data.DataLoader(trainSet, batch_size=batch_size, sampler=sampler, num_workers=2)\ntrain_loader_at_eval = torch.utils.data.DataLoader(trainSet_eval, batch_size=batch_size, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(testSet, batch_size=batch_size, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.027583Z","iopub.execute_input":"2023-05-08T01:03:21.029654Z","iopub.status.idle":"2023-05-08T01:03:21.044003Z","shell.execute_reply.started":"2023-05-08T01:03:21.029618Z","shell.execute_reply":"2023-05-08T01:03:21.042891Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.047762Z","iopub.execute_input":"2023-05-08T01:03:21.050543Z","iopub.status.idle":"2023-05-08T01:03:21.059576Z","shell.execute_reply.started":"2023-05-08T01:03:21.050512Z","shell.execute_reply":"2023-05-08T01:03:21.058618Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<torch.utils.data.dataloader.DataLoader object at 0x7abf6b611e40>\n","output_type":"stream"}]},{"cell_type":"code","source":"x, y = train_dataset[0]\n\nprint(x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.062397Z","iopub.execute_input":"2023-05-08T01:03:21.066556Z","iopub.status.idle":"2023-05-08T01:03:21.072012Z","shell.execute_reply.started":"2023-05-08T01:03:21.066526Z","shell.execute_reply":"2023-05-08T01:03:21.071080Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(1, 28, 28, 28) (1,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# frames = train_dataset.montage(length=20)\n# frames[10]","metadata":{"execution":{"iopub.status.busy":"2023-05-07T22:56:18.951041Z","iopub.execute_input":"2023-05-07T22:56:18.951839Z","iopub.status.idle":"2023-05-07T22:56:18.958472Z","shell.execute_reply.started":"2023-05-07T22:56:18.951786Z","shell.execute_reply":"2023-05-07T22:56:18.957510Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lables = train_dataset.labels\ncnt1 = 0\ncnt0 = 0\nfor i in range(len(lables)):\n  if lables[i][0] == 0:\n    cnt0+=1\n  else:\n    cnt1+=1\nprint(cnt0)\nprint(cnt1)\nprint(cnt0*100/(cnt0+cnt1))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.073806Z","iopub.execute_input":"2023-05-08T01:03:21.074802Z","iopub.status.idle":"2023-05-08T01:03:21.094126Z","shell.execute_reply.started":"2023-05-08T01:03:21.074769Z","shell.execute_reply":"2023-05-08T01:03:21.093059Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"331\n899\n26.910569105691057\n","output_type":"stream"}]},{"cell_type":"code","source":"\nlables = test_dataset.labels\ncnt1 = 0\ncnt0 = 0\nfor i in range(len(lables)):\n  if lables[i][0] == 0:\n    cnt0+=1\n  else:\n    cnt1+=1\nprint(cnt0)\nprint(cnt1)\nprint(cnt0*100/(cnt0+cnt1))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.095651Z","iopub.execute_input":"2023-05-08T01:03:21.099426Z","iopub.status.idle":"2023-05-08T01:03:21.109986Z","shell.execute_reply.started":"2023-05-08T01:03:21.099389Z","shell.execute_reply":"2023-05-08T01:03:21.109212Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"95\n257\n26.988636363636363\n","output_type":"stream"}]},{"cell_type":"code","source":"from libauc.losses import AUCMLoss, CrossEntropyLoss\nfrom libauc.optimizers import PESG, Adam\nfrom libauc.models import densenet121 as DenseNet121\n\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n# from torch.optim import Adam","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.111502Z","iopub.execute_input":"2023-05-08T01:03:21.112127Z","iopub.status.idle":"2023-05-08T01:03:21.142407Z","shell.execute_reply.started":"2023-05-08T01:03:21.112098Z","shell.execute_reply":"2023-05-08T01:03:21.141290Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def set_all_seeds(SEED):\n    # REPRODUCIBILITY\n    torch.manual_seed(SEED)\n    np.random.seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.145735Z","iopub.execute_input":"2023-05-08T01:03:21.146054Z","iopub.status.idle":"2023-05-08T01:03:21.159095Z","shell.execute_reply.started":"2023-05-08T01:03:21.146024Z","shell.execute_reply":"2023-05-08T01:03:21.158009Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"device = torch.device(0 if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.162569Z","iopub.execute_input":"2023-05-08T01:03:21.163417Z","iopub.status.idle":"2023-05-08T01:03:21.255540Z","shell.execute_reply.started":"2023-05-08T01:03:21.163379Z","shell.execute_reply":"2023-05-08T01:03:21.254403Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# m2 = models.video.r3d_18(pretrained=True)\n# m2","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.261029Z","iopub.execute_input":"2023-05-08T01:03:21.264033Z","iopub.status.idle":"2023-05-08T01:03:21.269114Z","shell.execute_reply.started":"2023-05-08T01:03:21.263995Z","shell.execute_reply":"2023-05-08T01:03:21.268293Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# paramaters\nSEED = 123\nBATCH_SIZE = 16\nlr = 3e-4\nweight_decay = 1e-5\nnum_classes=1\n\n# model\nmodel = models.video.r3d_18(pretrained=True)\ninput_channel = model.fc.in_features\n#for param in model.parameters():\n    #param.requires_grad = False\nmodel.fc = nn.Sequential(nn.Linear(input_channel, 128),\n                         nn.ReLU(),\n#                          nn.Dropout(p=0.2),\n                         nn.Linear(128, 32),\n                         nn.ReLU(),\n#                          nn.Dropout(p=0.1),\n                         nn.Linear(32, num_classes),\n                         nn.Sigmoid()\n                         )\n\n\nmodel = model.to(device)\n\n# create a binary cross-entropy loss function\ncriterion = nn.BCELoss()\noptimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n# training\nbest_val_auc = 0 \nfor epoch in range(10):\n    print(\"epoch: \", epoch+1)\n    for idx, (index, data, labels) in enumerate(train_loader):\n      train_data, train_labels = data, labels\n      train_data=train_data.repeat(1,3,1,1,1)\n      train_labels=train_labels.float()\n\n      train_data, train_labels  = train_data.to(device), train_labels.to(device)\n      # break\n      y_pred = model(train_data.float())\n      loss = criterion(y_pred, train_labels)\n      #print(\"Training Loss= \", loss.item())\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      \n    print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')  \n      # validation  \n      #if idx % 100 == 0:\n      #if (1):\n    model.eval()\n    with torch.no_grad():    \n         test_pred = []\n         test_true = [] \n         for jdx, (index, data, labels) in enumerate(train_loader_at_eval):\n             test_data, test_labels = data, labels\n             test_data=test_data.repeat(1,3,1,1,1)\n             test_labels=test_labels.float()\n             test_data = test_data.to(device)\n             y_pred = model(test_data.float())\n             test_pred.append(y_pred.cpu().detach().numpy())\n             test_true.append(test_labels.numpy())\n     \n         test_true = np.concatenate(test_true)\n         test_pred = np.concatenate(test_pred)\n         val_auc_mean =  roc_auc_score(test_true, test_pred) \n         #model.train\n         if best_val_auc < val_auc_mean:\n            best_val_auc = val_auc_mean\n            torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n         print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc) )","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:03:21.275406Z","iopub.execute_input":"2023-05-08T01:03:21.277570Z","iopub.status.idle":"2023-05-08T01:18:16.806218Z","shell.execute_reply.started":"2023-05-08T01:03:21.277539Z","shell.execute_reply":"2023-05-08T01:18:16.804380Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n100%|██████████| 127M/127M [00:00<00:00, 152MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"epoch:  1\nValidating epoch: 1 Loss: 0.69244784116745\nEpoch=0, BatchID=111, Val_AUC=0.7962, Best_Val_AUC=0.7962\nepoch:  2\nValidating epoch: 2 Loss: 0.7008397579193115\nEpoch=1, BatchID=111, Val_AUC=0.4811, Best_Val_AUC=0.7962\nepoch:  3\nValidating epoch: 3 Loss: 0.6926736235618591\nEpoch=2, BatchID=111, Val_AUC=0.5253, Best_Val_AUC=0.7962\nepoch:  4\nValidating epoch: 4 Loss: 0.6966897249221802\nEpoch=3, BatchID=111, Val_AUC=0.4381, Best_Val_AUC=0.7962\nepoch:  5\nValidating epoch: 5 Loss: 0.6932229995727539\nEpoch=4, BatchID=111, Val_AUC=0.6712, Best_Val_AUC=0.7962\nepoch:  6\nValidating epoch: 6 Loss: 0.6925530433654785\nEpoch=5, BatchID=111, Val_AUC=0.3724, Best_Val_AUC=0.7962\nepoch:  7\nValidating epoch: 7 Loss: 0.6915828585624695\nEpoch=6, BatchID=111, Val_AUC=0.6531, Best_Val_AUC=0.7962\nepoch:  8\nValidating epoch: 8 Loss: 0.6930869817733765\nEpoch=7, BatchID=111, Val_AUC=0.5729, Best_Val_AUC=0.7962\nepoch:  9\nValidating epoch: 9 Loss: 0.6930283904075623\nEpoch=8, BatchID=111, Val_AUC=0.4394, Best_Val_AUC=0.7962\nepoch:  10\nValidating epoch: 10 Loss: 0.7212545871734619\nEpoch=9, BatchID=111, Val_AUC=0.6061, Best_Val_AUC=0.7962\n","output_type":"stream"}]},{"cell_type":"code","source":"    print(f'Testing ...')  \n    PATH = 'ce_pretrained_model.pth' \n    model_state_dict = torch.load(PATH)\n    #model = MyModel()  # Create an instance of your model\n    model.load_state_dict(model_state_dict)  # Load the saved parameters into the model\n    best_val_auc=0.0\n    with torch.no_grad():    \n         test_pred = []\n         test_true = [] \n         for jdx, (index, data, labels) in enumerate(test_loader):\n             test_data, test_labels = data, labels\n             test_data=test_data.repeat(1,3,1,1,1)\n             test_labels=test_labels.float()\n             test_data = test_data.to(device)\n             y_pred = model(test_data.float())\n             test_pred.append(y_pred.cpu().detach().numpy())\n             test_true.append(test_labels.numpy())\n     \n         test_true = np.concatenate(test_true)\n         test_pred = np.concatenate(test_pred)\n         val_auc_mean =  roc_auc_score(test_true, test_pred) \n        # model.train\n        # if best_val_auc < val_auc_mean:\n        #    best_val_auc = val_auc_mean\n        #    #torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n         print('Val_AUC=%.4f'%( val_auc_mean))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:18:16.808169Z","iopub.execute_input":"2023-05-08T01:18:16.808993Z","iopub.status.idle":"2023-05-08T01:18:18.259771Z","shell.execute_reply.started":"2023-05-08T01:18:16.808949Z","shell.execute_reply":"2023-05-08T01:18:18.257909Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Testing ...\nVal_AUC=0.7581\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n# from torch.optim import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# parameters\nclass_id = 1\n\n# paramaters\nSEED = 123\n# BATCH_SIZE = 32 #[16, 32, 64, 128]\n# #imratio = train_dataset.imratio\n# lr = 0.05 # using smaller learning rate is better [0.001, 0.01, 0.05, 0.1]\n\n# lrs = [0.001, 0.01, 0.05, 0.1]\nlrs = [0.05]\n#BATCH_SIZEs = [16, 32, 64, 128]\nBATCH_SIZEs = [16]\n\nepoch_decay = 2e-3\nweight_decay = 1e-5\nmargin = 1.0\n\nfor lr in lrs:\n    print(\"------------------lr: {}--------------------\".format(lr))\n    for BATCH_SIZE in BATCH_SIZEs:\n        print(\"================ Batch Size: {} ===================\".format(BATCH_SIZE))\n        # model\n        set_all_seeds(SEED)\n        # model\n        model = models.video.r3d_18(pretrained=False)\n        input_channel = model.fc.in_features\n        #for param in model.parameters():\n            #param.requires_grad = False\n        model.fc = nn.Sequential(nn.Linear(input_channel, 128),\n                                 nn.ReLU(),\n#                                  nn.Dropout(p=0.2),\n                                 nn.Linear(128, 32),\n                                 nn.ReLU(),\n#                                  nn.Dropout(p=0.1),\n                                 nn.Linear(32, num_classes),\n                                 nn.Sigmoid()\n                                 )\n\n\n        model = model.to(device)\n\n        #model = models.resnet18(pretrained=False)\n        #\n        #model.fc = nn.Sequential(\n        #    nn.Linear(512, 1),\n        #    nn.Sigmoid()\n        #)\n        #model = model.cuda()\n\n\n        # load pretrained model\n        if True:\n          PATH = 'ce_pretrained_model.pth' \n          state_dict = torch.load(PATH)\n          state_dict.pop('classifier.weight', None)\n          state_dict.pop('classifier.bias', None) \n          model.load_state_dict(state_dict, strict=False)\n\n\n        # define loss & optimizer\n        loss_fn = AUCMLoss()\n        optimizer = PESG(model, \n                         loss_fn=loss_fn, \n                         lr=lr, \n                         margin=margin, \n                         epoch_decay=epoch_decay, \n                         weight_decay=weight_decay)\n        \n#         optimizer = Adam(model.parameters(), lr = lr, weight_decay= weight_decay)\n        lr_scheduler_opt = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1, verbose = True)\n#         scheduler_1 = CosineAnnealingLR(optimizer, 20, eta_min = 1e-08, verbose = True)\n        scheduler_2 = ReduceLROnPlateau(optimizer, patience=3,  verbose=True, factor=0.5, \n                              threshold=1e-03, min_lr=1e-08, mode = 'max')\n\n        best_val_auc = 0\n        for epoch in range(30):\n          for idx, (index, data, labels) in enumerate(train_loader):\n              train_data, train_labels = data, labels\n              train_data=train_data.repeat(1,3,1,1,1)\n              train_labels=train_labels.float()\n\n              train_data, train_labels = train_data.to(device), train_labels.to(device)\n              y_pred = model(train_data.float())\n#               y_pred = torch.sigmoid(y_pred)\n              loss = loss_fn(y_pred, train_labels)\n              optimizer.zero_grad()\n              loss.backward()\n              optimizer.step()\n\n              # validation\n          #if idx % 400 == 0:\n          print(f'Validating epoch: {epoch+1} Loss: {loss.item()}')\n          model.eval()\n          val_auc = 0\n          with torch.no_grad():    \n                test_pred = []\n                test_true = [] \n                for jdx, (index, data, labels) in enumerate(train_loader_at_eval):\n                    test_data, test_label = data, labels\n                    test_data=test_data.repeat(1,3,1,1,1)\n                    test_labels=test_labels.float()\n                    test_data = test_data.to(device)\n                    y_pred = model(test_data.float())\n                    test_pred.append(y_pred.cpu().detach().numpy())\n                    test_true.append(test_label.numpy())\n\n                test_true = np.concatenate(test_true)\n                test_pred = np.concatenate(test_pred)\n                val_auc =  roc_auc_score(test_true, test_pred) \n                #model.train(\n                if best_val_auc < val_auc:\n                   best_val_auc = val_auc\n                   torch.save(model.state_dict(), 'synapse3d_resnet3d_adam_0.1_SLR_RLRP.pth'.format(lr, BATCH_SIZE))\n\n          print ('Epoch={}, BatchID={}, Val_AUC={} lr={}'.format(epoch, idx, val_auc, optimizer.lr))\n          scheduler_2.step(val_auc)\n          lr_scheduler_opt.step()\n#           scheduler_1.step()\n        print ('Best Val_AUC is %.4f'%best_val_auc)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-08T01:18:18.262781Z","iopub.execute_input":"2023-05-08T01:18:18.263442Z","iopub.status.idle":"2023-05-08T02:15:46.469760Z","shell.execute_reply.started":"2023-05-08T01:18:18.263397Z","shell.execute_reply":"2023-05-08T02:15:46.468557Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"------------------lr: 0.05--------------------\n================ Batch Size: 16 ===================\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 1 Loss: 0.14658193290233612\nEpoch=0, BatchID=111, Val_AUC=0.836078811369509 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 2 Loss: 0.20213329792022705\nEpoch=1, BatchID=111, Val_AUC=0.7877906976744186 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 3 Loss: 0.33287981152534485\nEpoch=2, BatchID=111, Val_AUC=0.7094638242894058 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 4 Loss: 0.08774922788143158\nEpoch=3, BatchID=111, Val_AUC=0.8565891472868217 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 5 Loss: 0.014233488589525223\nEpoch=4, BatchID=111, Val_AUC=0.8546511627906976 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 6 Loss: 0.06452137231826782\nEpoch=5, BatchID=111, Val_AUC=0.8522286821705427 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 7 Loss: 0.05440283939242363\nEpoch=6, BatchID=111, Val_AUC=0.8598191214470284 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 8 Loss: 0.1548101156949997\nEpoch=7, BatchID=111, Val_AUC=0.8412467700258398 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 9 Loss: 0.05580776184797287\nEpoch=8, BatchID=111, Val_AUC=0.8564276485788114 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-02.\nValidating epoch: 10 Loss: 0.05356009304523468\nEpoch=9, BatchID=111, Val_AUC=0.8674095607235142 lr=0.05\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 11 Loss: 0.02304955944418907\nEpoch=10, BatchID=111, Val_AUC=0.8612726098191213 lr=0.005000000000000001\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 12 Loss: 0.055793263018131256\nEpoch=11, BatchID=111, Val_AUC=0.8615956072351422 lr=0.005000000000000001\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 13 Loss: -0.004461964592337608\nEpoch=12, BatchID=111, Val_AUC=0.8619186046511628 lr=0.005000000000000001\nAdjusting learning rate of group 0 to 5.0000e-03.\nValidating epoch: 14 Loss: -0.006040217354893684\nEpoch=13, BatchID=111, Val_AUC=0.8603036175710594 lr=0.005000000000000001\nEpoch 00014: reducing learning rate of group 0 to 2.5000e-03.\nAdjusting learning rate of group 0 to 2.5000e-03.\nValidating epoch: 15 Loss: -0.0018919864669442177\nEpoch=14, BatchID=111, Val_AUC=0.859657622739018 lr=0.0025000000000000005\nAdjusting learning rate of group 0 to 2.5000e-03.\nValidating epoch: 16 Loss: 0.0014009000733494759\nEpoch=15, BatchID=111, Val_AUC=0.8614341085271318 lr=0.0025000000000000005\nAdjusting learning rate of group 0 to 2.5000e-03.\nValidating epoch: 17 Loss: 0.06420236080884933\nEpoch=16, BatchID=111, Val_AUC=0.857719638242894 lr=0.0025000000000000005\nAdjusting learning rate of group 0 to 2.5000e-03.\nValidating epoch: 18 Loss: 0.04596542567014694\nEpoch=17, BatchID=111, Val_AUC=0.8588501291989665 lr=0.0025000000000000005\nEpoch 00018: reducing learning rate of group 0 to 1.2500e-03.\nAdjusting learning rate of group 0 to 1.2500e-03.\nValidating epoch: 19 Loss: 0.0012966906651854515\nEpoch=18, BatchID=111, Val_AUC=0.8591731266149871 lr=0.0012500000000000002\nAdjusting learning rate of group 0 to 1.2500e-03.\nValidating epoch: 20 Loss: -0.004682509228587151\nEpoch=19, BatchID=111, Val_AUC=0.8609496124031008 lr=0.0012500000000000002\nAdjusting learning rate of group 0 to 1.2500e-04.\nValidating epoch: 21 Loss: 0.009394921362400055\nEpoch=20, BatchID=111, Val_AUC=0.8611111111111112 lr=0.00012500000000000003\nAdjusting learning rate of group 0 to 1.2500e-04.\nValidating epoch: 22 Loss: -0.003403676673769951\nEpoch=21, BatchID=111, Val_AUC=0.8612726098191215 lr=0.00012500000000000003\nEpoch 00022: reducing learning rate of group 0 to 6.2500e-05.\nAdjusting learning rate of group 0 to 6.2500e-05.\nValidating epoch: 23 Loss: 0.023978475481271744\nEpoch=22, BatchID=111, Val_AUC=0.8612726098191215 lr=6.250000000000001e-05\nAdjusting learning rate of group 0 to 6.2500e-05.\nValidating epoch: 24 Loss: 0.0041704317554831505\nEpoch=23, BatchID=111, Val_AUC=0.8609496124031009 lr=6.250000000000001e-05\nAdjusting learning rate of group 0 to 6.2500e-05.\nValidating epoch: 25 Loss: -0.0026821400970220566\nEpoch=24, BatchID=111, Val_AUC=0.8609496124031009 lr=6.250000000000001e-05\nAdjusting learning rate of group 0 to 6.2500e-05.\nValidating epoch: 26 Loss: 0.03217379376292229\nEpoch=25, BatchID=111, Val_AUC=0.8609496124031009 lr=6.250000000000001e-05\nEpoch 00026: reducing learning rate of group 0 to 3.1250e-05.\nAdjusting learning rate of group 0 to 3.1250e-05.\nValidating epoch: 27 Loss: 0.025500217452645302\nEpoch=26, BatchID=111, Val_AUC=0.8609496124031009 lr=3.125000000000001e-05\nAdjusting learning rate of group 0 to 3.1250e-05.\nValidating epoch: 28 Loss: 0.0035038692876696587\nEpoch=27, BatchID=111, Val_AUC=0.8609496124031008 lr=3.125000000000001e-05\nAdjusting learning rate of group 0 to 3.1250e-05.\nValidating epoch: 29 Loss: 0.047412507236003876\nEpoch=28, BatchID=111, Val_AUC=0.8609496124031008 lr=3.125000000000001e-05\nAdjusting learning rate of group 0 to 3.1250e-05.\nValidating epoch: 30 Loss: 0.043604154139757156\nEpoch=29, BatchID=111, Val_AUC=0.8609496124031008 lr=3.125000000000001e-05\nEpoch 00030: reducing learning rate of group 0 to 1.5625e-05.\nAdjusting learning rate of group 0 to 1.5625e-06.\nBest Val_AUC is 0.8674\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Testing ...')  \n# PATH = 'aucm_trained_model_0.0003_32_scheduler_20epochs.pth' \nPATH = '/kaggle/working/synapse3d_resnet3d_adam_0.1_SLR_RLRP.pth'\nmodel_state_dict = torch.load(PATH)\n#model = MyModel()  # Create an instance of your model\nmodel.load_state_dict(model_state_dict)  # Load the saved parameters into the model\nbest_val_auc=0.0\nwith torch.no_grad():    \n     test_pred = []\n     test_true = [] \n     for jdx, (index, data, labels) in enumerate(test_loader):\n         test_data, test_labels = data, labels\n         test_data=test_data.repeat(1,3,1,1,1)\n         test_labels=test_labels.float()\n         test_data = test_data.to(device)\n         y_pred = model(test_data.float())\n         test_pred.append(y_pred.cpu().detach().numpy())\n         test_true.append(test_labels.numpy())\n \n     test_true = np.concatenate(test_true)\n     test_pred = np.concatenate(test_pred)\n     val_auc_mean =  roc_auc_score(test_true, test_pred) \n    # model.train\n    # if best_val_auc < val_auc_mean:\n    #    best_val_auc = val_auc_mean\n    #    #torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n     print('Val_AUC=%.4f'%( val_auc_mean))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T02:32:15.129572Z","iopub.execute_input":"2023-05-08T02:32:15.129968Z","iopub.status.idle":"2023-05-08T02:32:16.573677Z","shell.execute_reply.started":"2023-05-08T02:32:15.129934Z","shell.execute_reply":"2023-05-08T02:32:16.572557Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Testing ...\nVal_AUC=0.8632\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}